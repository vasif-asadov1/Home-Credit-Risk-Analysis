{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Home Credit Risk - Data Cleaning with Polars\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-summary: \"Show code\"\n",
    "    df-print: paged\n",
    "    embed-resources: true\n",
    "    toc: true\n",
    "    toc-location: body\n",
    "    toc-depth: 3\n",
    "execute:\n",
    "  echo: true\n",
    "  warning: false\n",
    "  message: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Environment Setup and Library Initialization**\n",
    "\n",
    "**Aim:** In this initial phase, we establish a robust and high-performance data science environment capable of handling large-scale datasets. We import Polars and DuckDB to serve as our primary data manipulation engines; their columnar memory format and multi-threaded execution offer significant speed advantages over standard Pandas when processing millions of rows.\n",
    "\n",
    "We also initialize our modeling and interpretability stack with LightGBM (a gradient boosting framework) and SHAP (SHapley Additive exPlanations), which will be essential for feature importance analysis later in the pipeline. Finally, we configure global settings - suppressing non-critical warnings and setting Plotly to render interactive charts directly within the notebook - to ensure a clean and efficient workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import polars.selectors as cs\n",
    "import plotly.graph_objects as go\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pio.renderers.default = \"notebook\"\n",
    "# Reset options (optional, to avoid messing up future prints)\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pl.read_parquet(\"C:/Users/asado/Desktop/Home Credit Dataset/Machine Learning/home_credit_train_final.parquet\")\n",
    "df = train_data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:36:05.087086Z",
     "iopub.status.busy": "2025-12-27T19:36:05.086372Z",
     "iopub.status.idle": "2025-12-27T19:36:05.109521Z",
     "shell.execute_reply": "2025-12-27T19:36:05.108800Z",
     "shell.execute_reply.started": "2025-12-27T19:36:05.087052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 146)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>SK_ID_CURR</th><th>TARGET</th><th>NAME_CONTRACT_TYPE</th><th>CODE_GENDER</th><th>FLAG_OWN_CAR</th><th>FLAG_OWN_REALTY</th><th>CNT_CHILDREN</th><th>AMT_INCOME_TOTAL</th><th>AMT_CREDIT</th><th>AMT_ANNUITY</th><th>AMT_GOODS_PRICE</th><th>NAME_TYPE_SUITE</th><th>NAME_INCOME_TYPE</th><th>NAME_EDUCATION_TYPE</th><th>NAME_FAMILY_STATUS</th><th>NAME_HOUSING_TYPE</th><th>REGION_POPULATION_RELATIVE</th><th>DAYS_BIRTH</th><th>DAYS_EMPLOYED</th><th>DAYS_REGISTRATION</th><th>DAYS_ID_PUBLISH</th><th>OWN_CAR_AGE</th><th>FLAG_MOBIL</th><th>FLAG_EMP_PHONE</th><th>FLAG_WORK_PHONE</th><th>FLAG_CONT_MOBILE</th><th>FLAG_PHONE</th><th>FLAG_EMAIL</th><th>OCCUPATION_TYPE</th><th>CNT_FAM_MEMBERS</th><th>REGION_RATING_CLIENT</th><th>REGION_RATING_CLIENT_W_CITY</th><th>WEEKDAY_APPR_PROCESS_START</th><th>HOUR_APPR_PROCESS_START</th><th>REG_REGION_NOT_LIVE_REGION</th><th>REG_REGION_NOT_WORK_REGION</th><th>LIVE_REGION_NOT_WORK_REGION</th><th>&hellip;</th><th>pos_max_dpd_def</th><th>pos_completed_rate</th><th>pos_active_rate</th><th>pos_term_variance</th><th>ins_late_payment_count</th><th>ins_avg_dpd</th><th>ins_max_dpd</th><th>ins_avg_dbd</th><th>ins_max_dbd</th><th>ins_underpayment_count</th><th>ins_payment_ratio</th><th>ins_total_missed_balance</th><th>ins_version_change_count</th><th>ins_recent_late_count_1y</th><th>bureau_total_loans_count</th><th>bureau_active_loans_count</th><th>bureau_closed_loans_count</th><th>bureau_credit_card_count</th><th>bureau_mortgage_count</th><th>bureau_car_loan_count</th><th>bureau_total_active_debt</th><th>bureau_total_active_limit</th><th>bureau_total_overdue_amount</th><th>bureau_max_overdue_ever</th><th>bureau_utilization_rate</th><th>bureau_days_since_last_loan</th><th>bureau_days_until_next_end</th><th>bureau_max_delinquency_level_ever</th><th>bureau_total_late_months_history</th><th>ratio_credit_to_income</th><th>ratio_annuity_to_income</th><th>ratio_annuity_to_credit</th><th>ratio_total_debt_to_income</th><th>ratio_total_credit_limit_to_income</th><th>feature_disposable_income</th><th>ratio_employed_to_age</th><th>ratio_income_to_age</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>&hellip;</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>i32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>384405</td><td>0</td><td>&quot;Cash loans&quot;</td><td>&quot;F&quot;</td><td>&quot;Y&quot;</td><td>&quot;Y&quot;</td><td>0</td><td>126000.0</td><td>780363.0</td><td>31077.0</td><td>697500.0</td><td>&quot;Family&quot;</td><td>&quot;Working&quot;</td><td>&quot;Secondary / secondary special&quot;</td><td>&quot;Married&quot;</td><td>&quot;House / apartment&quot;</td><td>0.032561</td><td>-18334</td><td>-2918</td><td>-1345.0</td><td>-1804</td><td>15.0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>&quot;High skill tech staff&quot;</td><td>2.0</td><td>1</td><td>1</td><td>&quot;THURSDAY&quot;</td><td>17</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0.05</td><td>0.95</td><td>2.260526</td><td>0.0</td><td>0.0</td><td>0.0</td><td>38.315789</td><td>71.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2</td><td>0.0</td><td>1</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.275228e6</td><td>1.35e6</td><td>0.0</td><td>null</td><td>0.944613</td><td>-308</td><td>null</td><td>null</td><td>0.0</td><td>6.193357</td><td>0.246643</td><td>0.039824</td><td>16.314214</td><td>16.907643</td><td>94923.0</td><td>0.159158</td><td>6.872477</td></tr><tr><td>384407</td><td>0</td><td>&quot;Cash loans&quot;</td><td>&quot;F&quot;</td><td>&quot;N&quot;</td><td>&quot;Y&quot;</td><td>0</td><td>112500.0</td><td>331834.5</td><td>20182.5</td><td>252000.0</td><td>&quot;Unaccompanied&quot;</td><td>&quot;Working&quot;</td><td>&quot;Secondary / secondary special&quot;</td><td>&quot;Civil marriage&quot;</td><td>&quot;House / apartment&quot;</td><td>0.005084</td><td>-11368</td><td>-2860</td><td>-3625.0</td><td>-3506</td><td>null</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>&quot;Managers&quot;</td><td>2.0</td><td>2</td><td>2</td><td>&quot;THURSDAY&quot;</td><td>10</td><td>1</td><td>1</td><td>0</td><td>&hellip;</td><td>0</td><td>0.166667</td><td>0.833333</td><td>4.69697</td><td>0.0</td><td>0.0</td><td>0.0</td><td>13.7</td><td>39.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2</td><td>0.0</td><td>3</td><td>3.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>0.0</td><td>173700.0</td><td>387000.0</td><td>0.0</td><td>null</td><td>0.448837</td><td>-60</td><td>1510.0</td><td>0</td><td>0.0</td><td>2.94964</td><td>0.1794</td><td>0.060821</td><td>4.49364</td><td>6.38964</td><td>92317.5</td><td>0.251583</td><td>9.8962</td></tr><tr><td>384408</td><td>0</td><td>&quot;Cash loans&quot;</td><td>&quot;F&quot;</td><td>&quot;N&quot;</td><td>&quot;Y&quot;</td><td>0</td><td>202500.0</td><td>545040.0</td><td>20677.5</td><td>450000.0</td><td>&quot;Unaccompanied&quot;</td><td>&quot;Working&quot;</td><td>&quot;Secondary / secondary special&quot;</td><td>&quot;Married&quot;</td><td>&quot;House / apartment&quot;</td><td>0.010276</td><td>-18286</td><td>-2789</td><td>-10980.0</td><td>-1821</td><td>null</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>&quot;Security staff&quot;</td><td>2.0</td><td>2</td><td>2</td><td>&quot;WEDNESDAY&quot;</td><td>13</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0.028571</td><td>0.971429</td><td>16.457143</td><td>1.0</td><td>0.058824</td><td>3.0</td><td>8.372549</td><td>60.0</td><td>2.0</td><td>1.152014</td><td>-174219.93</td><td>4</td><td>1.0</td><td>3</td><td>2.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>230184.0</td><td>396000.0</td><td>0.0</td><td>0.0</td><td>0.581273</td><td>-667</td><td>681.0</td><td>0</td><td>0.0</td><td>2.691556</td><td>0.102111</td><td>0.037938</td><td>3.828267</td><td>4.647111</td><td>181822.5</td><td>0.152521</td><td>11.074046</td></tr><tr><td>384409</td><td>0</td><td>&quot;Cash loans&quot;</td><td>&quot;M&quot;</td><td>&quot;Y&quot;</td><td>&quot;N&quot;</td><td>0</td><td>194121.0</td><td>1.35e6</td><td>48798.0</td><td>1.35e6</td><td>&quot;Unaccompanied&quot;</td><td>&quot;Commercial associate&quot;</td><td>&quot;Higher education&quot;</td><td>&quot;Civil marriage&quot;</td><td>&quot;House / apartment&quot;</td><td>0.030755</td><td>-14060</td><td>-948</td><td>-264.0</td><td>-4417</td><td>6.0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>&quot;Managers&quot;</td><td>2.0</td><td>2</td><td>2</td><td>&quot;MONDAY&quot;</td><td>18</td><td>1</td><td>1</td><td>1</td><td>&hellip;</td><td>0</td><td>0.1875</td><td>0.8125</td><td>78.929167</td><td>0.0</td><td>0.0</td><td>0.0</td><td>17.857143</td><td>37.0</td><td>0.0</td><td>1.510805</td><td>-102844.26</td><td>4</td><td>0.0</td><td>1</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-516</td><td>-455.0</td><td>null</td><td>0.0</td><td>6.954425</td><td>0.251379</td><td>0.036147</td><td>6.954425</td><td>6.954425</td><td>145323.0</td><td>0.067425</td><td>13.806615</td></tr><tr><td>384410</td><td>0</td><td>&quot;Cash loans&quot;</td><td>&quot;F&quot;</td><td>&quot;N&quot;</td><td>&quot;Y&quot;</td><td>0</td><td>112500.0</td><td>296505.0</td><td>23904.0</td><td>247500.0</td><td>&quot;Unaccompanied&quot;</td><td>&quot;Working&quot;</td><td>&quot;Secondary / secondary special&quot;</td><td>&quot;Married&quot;</td><td>&quot;House / apartment&quot;</td><td>0.035792</td><td>-17274</td><td>-614</td><td>-2674.0</td><td>-809</td><td>null</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>&quot;Sales staff&quot;</td><td>2.0</td><td>2</td><td>2</td><td>&quot;TUESDAY&quot;</td><td>11</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0.076923</td><td>0.923077</td><td>0.0</td><td>6.0</td><td>0.433962</td><td>6.0</td><td>10.830189</td><td>40.0</td><td>10.0</td><td>0.954487</td><td>9044.19</td><td>4</td><td>4.0</td><td>2</td><td>0.0</td><td>2.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>-524</td><td>1303.0</td><td>1</td><td>4.0</td><td>2.6356</td><td>0.21248</td><td>0.080619</td><td>2.6356</td><td>2.6356</td><td>88596.0</td><td>0.035545</td><td>6.512678</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 146)\n",
       "┌────────────┬────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ SK_ID_CURR ┆ TARGET ┆ NAME_CONTR ┆ CODE_GEND ┆ … ┆ ratio_tot ┆ feature_d ┆ ratio_emp ┆ ratio_inc │\n",
       "│ ---        ┆ ---    ┆ ACT_TYPE   ┆ ER        ┆   ┆ al_credit ┆ isposable ┆ loyed_to_ ┆ ome_to_ag │\n",
       "│ i64        ┆ i64    ┆ ---        ┆ ---       ┆   ┆ _limit_to ┆ _income   ┆ age       ┆ e         │\n",
       "│            ┆        ┆ str        ┆ str       ┆   ┆ _in…      ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆        ┆            ┆           ┆   ┆ ---       ┆ f64       ┆ f64       ┆ f64       │\n",
       "│            ┆        ┆            ┆           ┆   ┆ f64       ┆           ┆           ┆           │\n",
       "╞════════════╪════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 384405     ┆ 0      ┆ Cash loans ┆ F         ┆ … ┆ 16.907643 ┆ 94923.0   ┆ 0.159158  ┆ 6.872477  │\n",
       "│ 384407     ┆ 0      ┆ Cash loans ┆ F         ┆ … ┆ 6.38964   ┆ 92317.5   ┆ 0.251583  ┆ 9.8962    │\n",
       "│ 384408     ┆ 0      ┆ Cash loans ┆ F         ┆ … ┆ 4.647111  ┆ 181822.5  ┆ 0.152521  ┆ 11.074046 │\n",
       "│ 384409     ┆ 0      ┆ Cash loans ┆ M         ┆ … ┆ 6.954425  ┆ 145323.0  ┆ 0.067425  ┆ 13.806615 │\n",
       "│ 384410     ┆ 0      ┆ Cash loans ┆ F         ┆ … ┆ 2.6356    ┆ 88596.0   ┆ 0.035545  ┆ 6.512678  │\n",
       "└────────────┴────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Selection via SHAP Values**\n",
    "Aim In this step, I want to filter out the \"noise\" from the dataset before doing deeper cleaning. Since we have a lot of columns, I need to know which ones actually matter for predicting the target. To do this, I'm using a technique called **\"Feature Selection by Model\"**.\n",
    "\n",
    "I will train a quick LightGBM classifier on the data. Then, I'll use SHAP (SHapley Additive exPlanations) values to calculate exactly how much each feature contributes to the model's decisions. Using a sample of 150,000 rows (to keep it fast), I can identify features that have effectively zero impact on the prediction. My goal is to create a cols_to_drop list containing these useless features so I can remove them and focus only on the data that has signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:36:07.476067Z",
     "iopub.status.busy": "2025-12-27T19:36:07.475777Z",
     "iopub.status.idle": "2025-12-27T19:37:39.602279Z",
     "shell.execute_reply": "2025-12-27T19:37:39.601341Z",
     "shell.execute_reply.started": "2025-12-27T19:36:07.476039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Training model...\n",
      "Calculating SHAP values on 150,000 rows...\n",
      "\n",
      "==================================================\n",
      "FULL FEATURE IMPORTANCE LIST (144 Features)\n",
      "==================================================\n",
      "                                Feature  SHAP_Importance         Status\n",
      "40                         EXT_SOURCE_2         0.298252    Significant\n",
      "41                         EXT_SOURCE_3         0.290905    Significant\n",
      "39                         EXT_SOURCE_1         0.137472    Significant\n",
      "1                           CODE_GENDER         0.111131    Significant\n",
      "16                        DAYS_EMPLOYED         0.093163    Significant\n",
      "138             ratio_annuity_to_credit         0.079712    Significant\n",
      "11                  NAME_EDUCATION_TYPE         0.074473    Significant\n",
      "7                           AMT_ANNUITY         0.072939    Significant\n",
      "102            pos_total_months_history         0.072916    Significant\n",
      "120            ins_recent_late_count_1y         0.070148    Significant\n",
      "112                         ins_avg_dpd         0.065217    Significant\n",
      "19                          OWN_CAR_AGE         0.062132    Significant\n",
      "76             degree_of_trust_avg_prev         0.059121    Significant\n",
      "131             bureau_utilization_rate         0.058858    Significant\n",
      "8                       AMT_GOODS_PRICE         0.055557    Significant\n",
      "15                           DAYS_BIRTH         0.050442    Significant\n",
      "12                   NAME_FAMILY_STATUS         0.043099    Significant\n",
      "123           bureau_closed_loans_count         0.040360    Significant\n",
      "79               avg_term_approved_prev         0.040145    Significant\n",
      "104            pos_remaining_debt_ratio         0.039059    Significant\n",
      "75     days_since_last_termination_prev         0.038219    Significant\n",
      "81        avg_goods_price_approved_prev         0.036073    Significant\n",
      "18                      DAYS_ID_PUBLISH         0.035992    Significant\n",
      "72           high_yield_group_rate_prev         0.034203    Significant\n",
      "130             bureau_max_overdue_ever         0.033275    Significant\n",
      "122           bureau_active_loans_count         0.031729    Significant\n",
      "6                            AMT_CREDIT         0.031037    Significant\n",
      "96             cc_avg_monthly_atm_count         0.030937    Significant\n",
      "65                 total_document_count         0.030596    Significant\n",
      "117                   ins_payment_ratio         0.029524    Significant\n",
      "67                    refusal_rate_prev         0.028880    Significant\n",
      "137             ratio_annuity_to_income         0.028215    Significant\n",
      "132         bureau_days_since_last_loan         0.028049    Significant\n",
      "110                   pos_term_variance         0.027227    Significant\n",
      "74         days_since_last_refusal_prev         0.027209    Significant\n",
      "38                    ORGANIZATION_TYPE         0.026730    Significant\n",
      "133          bureau_days_until_next_end         0.025398    Significant\n",
      "29          REGION_RATING_CLIENT_W_CITY         0.024716    Significant\n",
      "10                     NAME_INCOME_TYPE         0.023779    Significant\n",
      "103         pos_avg_future_installments         0.023025    Significant\n",
      "78                max_prev_annuity_prev         0.022178    Significant\n",
      "48             DEF_30_CNT_SOCIAL_CIRCLE         0.021182    Significant\n",
      "64                 flag_document_3_kept         0.020518    Significant\n",
      "95        cc_avg_monthly_drawings_count         0.020341    Significant\n",
      "89              cc_utilization_rate_avg         0.020115    Significant\n",
      "125               bureau_mortgage_count         0.019529    Significant\n",
      "140  ratio_total_credit_limit_to_income         0.018995    Significant\n",
      "142               ratio_employed_to_age         0.018875    Significant\n",
      "44                       TOTALAREA_MODE         0.018046    Significant\n",
      "107                     pos_max_dpd_def         0.016355    Significant\n",
      "77           avg_down_payment_rate_prev         0.015444    Significant\n",
      "118            ins_total_missed_balance         0.015205    Significant\n",
      "51               DAYS_LAST_PHONE_CHANGE         0.014960    Significant\n",
      "68                   approval_rate_prev         0.014843    Significant\n",
      "109                     pos_active_rate         0.013976    Significant\n",
      "128           bureau_total_active_limit         0.013059    Significant\n",
      "26                      OCCUPATION_TYPE         0.011504    Significant\n",
      "35               REG_CITY_NOT_LIVE_CITY         0.011372    Significant\n",
      "80         avg_goods_price_refused_prev         0.011192    Significant\n",
      "62                   housing_score_medi         0.010928    Significant\n",
      "22                      FLAG_WORK_PHONE         0.010213    Significant\n",
      "56            AMT_REQ_CREDIT_BUREAU_QRT         0.009776    Significant\n",
      "73        days_since_last_approval_prev         0.009053    Significant\n",
      "127            bureau_total_active_debt         0.008574    Significant\n",
      "0                    NAME_CONTRACT_TYPE         0.008280    Significant\n",
      "139          ratio_total_debt_to_income         0.008263    Significant\n",
      "17                    DAYS_REGISTRATION         0.008051    Significant\n",
      "70                    walk_in_rate_prev         0.007736    Significant\n",
      "14           REGION_POPULATION_RELATIVE         0.007039    Significant\n",
      "136              ratio_credit_to_income         0.006892    Significant\n",
      "92            cc_payment_over_min_ratio         0.006840    Significant\n",
      "50             DEF_60_CNT_SOCIAL_CIRCLE         0.006662    Significant\n",
      "108                  pos_completed_rate         0.004809    Significant\n",
      "24                           FLAG_PHONE         0.004494    Significant\n",
      "115                         ins_max_dbd         0.004323    Significant\n",
      "143                 ratio_income_to_age         0.004321    Significant\n",
      "119            ins_version_change_count         0.004279    Significant\n",
      "69                  cash_loan_rate_prev         0.004177    Significant\n",
      "90               cc_balance_spike_ratio         0.003288    Significant\n",
      "87               cc_active_months_count         0.003281    Significant\n",
      "60                   housing_score_mode         0.003162    Significant\n",
      "134   bureau_max_delinquency_level_ever         0.003101    Significant\n",
      "2                          FLAG_OWN_CAR         0.003076    Significant\n",
      "30           WEEKDAY_APPR_PROCESS_START         0.003054    Significant\n",
      "113                         ins_max_dpd         0.002741    Significant\n",
      "86              cc_total_months_history         0.002708    Significant\n",
      "126               bureau_car_loan_count         0.002682    Significant\n",
      "98                           cc_max_dpd         0.002656    Significant\n",
      "124            bureau_credit_card_count         0.002573    Significant\n",
      "58                    housing_score_avg         0.002327    Significant\n",
      "114                         ins_avg_dbd         0.002280    Significant\n",
      "31              HOUR_APPR_PROCESS_START         0.002047    Significant\n",
      "129         bureau_total_overdue_amount         0.001895    Significant\n",
      "100               cc_late_payment_count         0.001605    Significant\n",
      "141           feature_disposable_income         0.001486    Significant\n",
      "106                         pos_max_dpd         0.001412    Significant\n",
      "97             cc_avg_monthly_pos_count         0.001360    Significant\n",
      "5                      AMT_INCOME_TOTAL         0.001208    Significant\n",
      "91               cc_avg_interest_burden         0.001108    Significant\n",
      "66               application_count_prev         0.001084    Significant\n",
      "121            bureau_total_loans_count         0.001065    Significant\n",
      "94                   cc_avg_ticket_size         0.001015    Significant\n",
      "49             OBS_60_CNT_SOCIAL_CIRCLE         0.000987    Significant\n",
      "28                 REGION_RATING_CLIENT         0.000912    Significant\n",
      "93                  cc_atm_drawing_rate         0.000785    Significant\n",
      "84       most_freq_seller_industry_prev         0.000773    Significant\n",
      "105              pos_late_payment_count         0.000768    Significant\n",
      "71           insurance_uptake_rate_prev         0.000738    Significant\n",
      "111              ins_late_payment_count         0.000672    Significant\n",
      "3                       FLAG_OWN_REALTY         0.000656    Significant\n",
      "101               cc_limit_growth_value         0.000487    Significant\n",
      "4                          CNT_CHILDREN         0.000456    Significant\n",
      "27                      CNT_FAM_MEMBERS         0.000232    Significant\n",
      "43                       HOUSETYPE_MODE         0.000224    Significant\n",
      "53            AMT_REQ_CREDIT_BUREAU_DAY         0.000156    Significant\n",
      "42                   FONDKAPREMONT_MODE         0.000145    Significant\n",
      "13                    NAME_HOUSING_TYPE         0.000123    Significant\n",
      "135    bureau_total_late_months_history         0.000121    Significant\n",
      "59                   housing_data_count         0.000113    Significant\n",
      "116              ins_underpayment_count         0.000110    Significant\n",
      "45                   WALLSMATERIAL_MODE         0.000108    Significant\n",
      "83      avg_rate_interest_approved_prev         0.000098    Significant\n",
      "88            cc_completed_months_count         0.000092    Significant\n",
      "9                       NAME_TYPE_SUITE         0.000081    Significant\n",
      "47             OBS_30_CNT_SOCIAL_CIRCLE         0.000081    Significant\n",
      "57           AMT_REQ_CREDIT_BUREAU_YEAR         0.000077    Significant\n",
      "33           REG_REGION_NOT_WORK_REGION         0.000000  Insignificant\n",
      "46                  EMERGENCYSTATE_MODE         0.000000  Insignificant\n",
      "55            AMT_REQ_CREDIT_BUREAU_MON         0.000000  Insignificant\n",
      "54           AMT_REQ_CREDIT_BUREAU_WEEK         0.000000  Insignificant\n",
      "61              housing_data_count_mode         0.000000  Insignificant\n",
      "52           AMT_REQ_CREDIT_BUREAU_HOUR         0.000000  Insignificant\n",
      "63              housing_data_count_medi         0.000000  Insignificant\n",
      "37              LIVE_CITY_NOT_WORK_CITY         0.000000  Insignificant\n",
      "21                       FLAG_EMP_PHONE         0.000000  Insignificant\n",
      "23                     FLAG_CONT_MOBILE         0.000000  Insignificant\n",
      "25                           FLAG_EMAIL         0.000000  Insignificant\n",
      "20                           FLAG_MOBIL         0.000000  Insignificant\n",
      "36               REG_CITY_NOT_WORK_CITY         0.000000  Insignificant\n",
      "34          LIVE_REGION_NOT_WORK_REGION         0.000000  Insignificant\n",
      "32           REG_REGION_NOT_LIVE_REGION         0.000000  Insignificant\n",
      "99                       cc_max_dpd_def         0.000000  Insignificant\n",
      "85           last_rejection_reason_prev         0.000000  Insignificant\n",
      "82       avg_rate_interest_refused_prev         0.000000  Insignificant\n",
      "\n",
      "==================================================\n",
      "INSIGNIFICANT FEATURES TO DROP (18)\n",
      "==================================================\n",
      "['REG_REGION_NOT_WORK_REGION', 'EMERGENCYSTATE_MODE', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'housing_data_count_mode', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'housing_data_count_medi', 'LIVE_CITY_NOT_WORK_CITY', 'FLAG_EMP_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'FLAG_MOBIL', 'REG_CITY_NOT_WORK_CITY', 'LIVE_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_LIVE_REGION', 'cc_max_dpd_def', 'last_rejection_reason_prev', 'avg_rate_interest_refused_prev']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/shap/explainers/_tree.py:586: UserWarning:\n",
      "\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. PREPARE DATA\n",
    "print(\"Preparing data...\")\n",
    "drop_cols = [\"SK_ID_CURR\", \"TARGET\"]\n",
    "feature_names = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "# Convert to Pandas\n",
    "train_df = df.select(feature_names + [\"TARGET\"]).to_pandas()\n",
    "X = train_df[feature_names].copy()\n",
    "y = train_df[\"TARGET\"]\n",
    "\n",
    "# Encode Categorical\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "# 2. TRAIN MODEL\n",
    "print(\"Training model...\")\n",
    "model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. CALCULATE SHAP (150,000 Rows)\n",
    "print(\"Calculating SHAP values on 150,000 rows...\")\n",
    "sample_size = 150000\n",
    "if len(X) > sample_size:\n",
    "    X_sample = X.sample(n=sample_size, random_state=42)\n",
    "else:\n",
    "    X_sample = X.copy()\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "if isinstance(shap_values, list):\n",
    "    shap_vals_target = shap_values[1]\n",
    "else:\n",
    "    shap_vals_target = shap_values\n",
    "\n",
    "# 4. CREATE DATAFRAME\n",
    "mean_shap = np.abs(shap_vals_target).mean(axis=0)\n",
    "\n",
    "significance_df = pd.DataFrame({\n",
    "    'Feature': X_sample.columns,\n",
    "    'SHAP_Importance': mean_shap\n",
    "})\n",
    "\n",
    "# Threshold\n",
    "threshold = 1e-5\n",
    "significance_df['Status'] = np.where(\n",
    "    significance_df['SHAP_Importance'] >= threshold, \n",
    "    'Significant', \n",
    "    'Insignificant'\n",
    ")\n",
    "\n",
    "# Sort\n",
    "significance_df = significance_df.sort_values(by='SHAP_Importance', ascending=False)\n",
    "significance_df['SHAP_Importance'] = significance_df['SHAP_Importance'].round(6)\n",
    "\n",
    "# 5. PRINT FULL DATAFRAME (ALL ROWS VISIBLE)\n",
    "# This forces Pandas to display everything\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"FULL FEATURE IMPORTANCE LIST ({len(significance_df)} Features)\")\n",
    "print(\"=\"*50)\n",
    "print(significance_df)\n",
    "\n",
    "# 6. PRINT DROP LIST\n",
    "cols_to_drop = significance_df[significance_df['Status'] == 'Insignificant']['Feature'].tolist()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"INSIGNIFICANT FEATURES TO DROP ({len(cols_to_drop)})\")\n",
    "print(\"=\"*50)\n",
    "print(cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHAP Feature Importance Analysis Results**\n",
    "\n",
    "**Interpretation**\n",
    "The SHAP analysis provides a \"global view\" of what drives our model's decisions. By aggregating the absolute SHAP values across all samples, we can see which features actually move the needle on probability of default.\n",
    "\n",
    "Here is the breakdown of the signal we found:\n",
    "\n",
    "**1. The \"Big Three\" Dominance (External Sources)**\n",
    "The top three features are exclusively `EXT_SOURCE_1`, `EXT_SOURCE_2`, and `EXT_SOURCE_3`.\n",
    "\n",
    "* **What this means:** The model relies heavily on these external credit scores (likely from other credit bureaus like Equifax or Experian).\n",
    "* **Insight:** This is expected in credit risk. Past credit behavior (captured in these scores) is the single best predictor of future behavior. `EXT_SOURCE_2` alone has an importance of `0.29`, which is nearly **3x more powerful** than the next best feature type.\n",
    "\n",
    "**2. Demographics and Stability**\n",
    "\n",
    "* **`CODE_GENDER` (Rank #4):** This is surprisingly high (`0.11`). It suggests a strong statistical difference in default rates between men and women in this dataset.\n",
    "* **`DAYS_EMPLOYED` (Rank #5) & `DAYS_BIRTH`:** Job stability and age are critical. The model likely sees that older applicants with longer tenure are safer borrowers.\n",
    "* **`NAME_EDUCATION_TYPE`:** Education level is a key proxy for income potential and financial literacy.\n",
    "\n",
    "**3. The Power of Ratios (Feature Engineering)**\n",
    "\n",
    "* **`ratio_annuity_to_credit` (Rank #6):** This is a derived feature (not raw data), and its high ranking validates our feature engineering. It measures \"affordability\"—how large the monthly payment is relative to the total loan. A high ratio usually indicates a high burden, increasing default risk.\n",
    "\n",
    "**4. The \"Zero Signal\" Features (The Drop List)**\n",
    "Crucially, we identified a block of features with **0.000000** importance. These are the \"Insignificant\" features we will drop.\n",
    "\n",
    "* **Contact Flags (`FLAG_MOBIL`, `FLAG_EMAIL`, `FLAG_CONT_MOBILE`):** These failed because they lack variance. In the modern era, nearly everyone provides a mobile number. If 99.9% of rows have a \"1\", the model cannot use it to separate good from bad borrowers.\n",
    "* **Region Mismatches (`REG_REGION_NOT_WORK_REGION`, `LIVE_CITY_NOT_WORK_CITY`):** The model found no correlation between living in a different city than you work and defaulting. This is just noise.\n",
    "* **Granular Bureau Enquiries (`AMT_REQ_CREDIT_BUREAU_HOUR/WEEK`):** While the number of enquiries *total* might matter, the model decided that knowing if an enquiry happened \"in the last hour\" specifically is irrelevant compared to the bigger picture.\n",
    "\n",
    "**Conclusion**\n",
    "We have successfully separated the signal from the noise. We can safely drop the 19 features at the bottom (flags, address mismatches, and granular bureau times) without hurting the model's accuracy. In fact, removing them will likely make the model more robust by reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Execution of Feature Dropping**\n",
    "\n",
    "**Aim:**\n",
    "Now that we have a confirmed list of \"noise\" features (`cols_to_drop`) from our SHAP analysis, the objective here is to physically remove them from the dataset. This is a critical efficiency step: carrying 19+ useless columns consumes memory and computation power for every subsequent operation (joins, aggregations, training) without adding any value.\n",
    "\n",
    "Crucially, this block acts as a **safety checkpoint**. Automated cleaning pipelines can sometimes be too aggressive. Therefore, the code explicitly verifies two things:\n",
    "\n",
    "1. **Dimensionality Reduction:** We print the new shape to confirm the columns were actually deleted (the column count should decrease).\n",
    "2. **Target Integrity:** We check if the `TARGET` column still exists. If we accidentally drop the column we are trying to predict, the entire dataset becomes useless for supervised learning. This \"unit test\" prevents us from moving forward with a broken dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:37:52.168523Z",
     "iopub.status.busy": "2025-12-27T19:37:52.167857Z",
     "iopub.status.idle": "2025-12-27T19:37:52.174984Z",
     "shell.execute_reply": "2025-12-27T19:37:52.174210Z",
     "shell.execute_reply.started": "2025-12-27T19:37:52.168461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 18 insignificant features...\n",
      "Drop complete.\n",
      "\n",
      "--- DATASET STATUS ---\n",
      "New Shape: (307511, 128)\n",
      "Remaining Columns: 128\n",
      "------------------------------\n",
      "Target column is present. Ready for modeling.\n"
     ]
    }
   ],
   "source": [
    "# 1. Execute Drop\n",
    "# We check if the list exists and has items to avoid errors\n",
    "if 'cols_to_drop' in locals() and len(cols_to_drop) > 0:\n",
    "    print(f\"Dropping {len(cols_to_drop)} insignificant features...\")\n",
    "    \n",
    "    # Polars drop operation\n",
    "    df = df.drop(cols_to_drop)\n",
    "    \n",
    "    print(\"Drop complete.\")\n",
    "else:\n",
    "    print(\"'cols_to_drop' variable not found or empty. Please ensure the SHAP analysis code ran successfully.\")\n",
    "\n",
    "# 2. Verify New Dataset Dimensions\n",
    "print(f\"\\n--- DATASET STATUS ---\")\n",
    "print(f\"New Shape: {df.shape}\")\n",
    "print(f\"Remaining Columns: {len(df.columns)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 3. Quick Check: Ensure Target is still there\n",
    "if \"TARGET\" in df.columns:\n",
    "    print(\"Target column is present. Ready for modeling.\")\n",
    "else:\n",
    "    print(\"ALARM: Target column was accidentally dropped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output confirms that the cleaning operation was successful and safe.\n",
    "\n",
    "* **Dimensionality Reduction:** We successfully removed **18 features**. We have slimmed the dataset down to **128 columns**. This might seem small, but removing 18 columns across 300,000+ rows saves significant memory and will speed up every future calculation (like the missing value check we are about to do).\n",
    "* **Safety Verification:** Most importantly, the system returned **\"Target column is present.\"** This is our \"Green Light.\" It means we haven't broken the dataset. We still have our labels (0 and 1) and our reduced set of features, so we are cleared to proceed to the next stage of data cleaning.\n",
    "\n",
    "**Next Step:**\n",
    "Now that we have removed the *useless* columns, we need to look at the *broken* ones—specifically, columns that are full of missing values (nulls). Would you like to proceed with the **Missing Value Analysis** code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Missing Value Analysis (Missingness Report)**\n",
    "\n",
    "**Aim:** Now that we've removed the useless features, we need to find the \"broken\" ones. Machine learning models (like LightGBM or XGBoost) can handle some missing data, but too much missingness can ruin predictions.\n",
    "\n",
    "My objective here is to generate a comprehensive \"Health Report\" for the dataset. I don't just want the count of missing values (e.g., \"50,000 nulls\"); I want the percentage.\n",
    "\n",
    "If a column is 5% missing, I can fix it (impute with mean/median).\n",
    "\n",
    "If a column is 95% missing, it's mostly empty space and likely needs to be dropped.\n",
    "\n",
    "This code scans every single column, counts the nulls, and calculates a percentage. It then sorts the list so the \"emptiest\" columns appear at the top, allowing me to instantly see which features are in the \"Danger Zone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:38:01.109085Z",
     "iopub.status.busy": "2025-12-27T19:38:01.108436Z",
     "iopub.status.idle": "2025-12-27T19:38:01.158803Z",
     "shell.execute_reply": "2025-12-27T19:38:01.158024Z",
     "shell.execute_reply.started": "2025-12-27T19:38:01.109052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Missing Value Report (Total Rows: 307511) ---\n",
      "Columns with missing values: 96\n",
      "shape: (96, 3)\n",
      "┌─────────────────────────────────┬────────────┬──────────────┐\n",
      "│ Feature                         ┆ Null_Count ┆ Null_Percent │\n",
      "│ ---                             ┆ ---        ┆ ---          │\n",
      "│ str                             ┆ u32        ┆ f64          │\n",
      "╞═════════════════════════════════╪════════════╪══════════════╡\n",
      "│ avg_rate_interest_approved_pre… ┆ 302902     ┆ 98.5         │\n",
      "│ cc_avg_ticket_size              ┆ 248339     ┆ 80.76        │\n",
      "│ cc_payment_over_min_ratio       ┆ 248232     ┆ 80.72        │\n",
      "│ cc_balance_spike_ratio          ┆ 247911     ┆ 80.62        │\n",
      "│ cc_atm_drawing_rate             ┆ 247875     ┆ 80.61        │\n",
      "│ cc_avg_monthly_atm_count        ┆ 246371     ┆ 80.12        │\n",
      "│ cc_avg_monthly_pos_count        ┆ 246371     ┆ 80.12        │\n",
      "│ cc_utilization_rate_avg         ┆ 221475     ┆ 72.02        │\n",
      "│ cc_total_months_history         ┆ 220606     ┆ 71.74        │\n",
      "│ cc_active_months_count          ┆ 220606     ┆ 71.74        │\n",
      "│ cc_completed_months_count       ┆ 220606     ┆ 71.74        │\n",
      "│ cc_avg_interest_burden          ┆ 220606     ┆ 71.74        │\n",
      "│ cc_avg_monthly_drawings_count   ┆ 220606     ┆ 71.74        │\n",
      "│ cc_max_dpd                      ┆ 220606     ┆ 71.74        │\n",
      "│ cc_late_payment_count           ┆ 220606     ┆ 71.74        │\n",
      "│ cc_limit_growth_value           ┆ 220606     ┆ 71.74        │\n",
      "│ bureau_max_delinquency_level_e… ┆ 215280     ┆ 70.01        │\n",
      "│ avg_goods_price_refused_prev    ┆ 214278     ┆ 69.68        │\n",
      "│ FONDKAPREMONT_MODE              ┆ 210295     ┆ 68.39        │\n",
      "│ days_since_last_refusal_prev    ┆ 207217     ┆ 67.39        │\n",
      "│ OWN_CAR_AGE                     ┆ 202929     ┆ 65.99        │\n",
      "│ EXT_SOURCE_1                    ┆ 173378     ┆ 56.38        │\n",
      "│ WALLSMATERIAL_MODE              ┆ 156341     ┆ 50.84        │\n",
      "│ HOUSETYPE_MODE                  ┆ 154297     ┆ 50.18        │\n",
      "│ housing_score_avg               ┆ 148810     ┆ 48.39        │\n",
      "│ housing_score_mode              ┆ 148810     ┆ 48.39        │\n",
      "│ housing_score_medi              ┆ 148810     ┆ 48.39        │\n",
      "│ TOTALAREA_MODE                  ┆ 148431     ┆ 48.27        │\n",
      "│ bureau_max_overdue_ever         ┆ 123625     ┆ 40.2         │\n",
      "│ OCCUPATION_TYPE                 ┆ 96391      ┆ 31.35        │\n",
      "│ bureau_utilization_rate         ┆ 95419      ┆ 31.03        │\n",
      "│ EXT_SOURCE_3                    ┆ 60965      ┆ 19.83        │\n",
      "│ DAYS_EMPLOYED                   ┆ 55374      ┆ 18.01        │\n",
      "│ ratio_employed_to_age           ┆ 55374      ┆ 18.01        │\n",
      "│ bureau_days_until_next_end      ┆ 46269      ┆ 15.05        │\n",
      "│ bureau_total_active_debt        ┆ 46010      ┆ 14.96        │\n",
      "│ bureau_total_active_limit       ┆ 44021      ┆ 14.32        │\n",
      "│ bureau_total_loans_count        ┆ 44020      ┆ 14.31        │\n",
      "│ bureau_active_loans_count       ┆ 44020      ┆ 14.31        │\n",
      "│ bureau_closed_loans_count       ┆ 44020      ┆ 14.31        │\n",
      "│ bureau_credit_card_count        ┆ 44020      ┆ 14.31        │\n",
      "│ bureau_mortgage_count           ┆ 44020      ┆ 14.31        │\n",
      "│ bureau_car_loan_count           ┆ 44020      ┆ 14.31        │\n",
      "│ bureau_total_overdue_amount     ┆ 44020      ┆ 14.31        │\n",
      "│ bureau_days_since_last_loan     ┆ 44020      ┆ 14.31        │\n",
      "│ bureau_total_late_months_histo… ┆ 44020      ┆ 14.31        │\n",
      "│ AMT_REQ_CREDIT_BUREAU_DAY       ┆ 41519      ┆ 13.5         │\n",
      "│ AMT_REQ_CREDIT_BUREAU_QRT       ┆ 41519      ┆ 13.5         │\n",
      "│ AMT_REQ_CREDIT_BUREAU_YEAR      ┆ 41519      ┆ 13.5         │\n",
      "│ days_since_last_termination_pr… ┆ 40497      ┆ 13.17        │\n",
      "│ avg_down_payment_rate_prev      ┆ 33906      ┆ 11.03        │\n",
      "│ degree_of_trust_avg_prev        ┆ 18467      ┆ 6.01         │\n",
      "│ pos_term_variance               ┆ 18437      ┆ 6.0          │\n",
      "│ avg_goods_price_approved_prev   ┆ 18425      ┆ 5.99         │\n",
      "│ pos_total_months_history        ┆ 18067      ┆ 5.88         │\n",
      "│ pos_avg_future_installments     ┆ 18091      ┆ 5.88         │\n",
      "│ pos_remaining_debt_ratio        ┆ 18091      ┆ 5.88         │\n",
      "│ pos_late_payment_count          ┆ 18067      ┆ 5.88         │\n",
      "│ pos_max_dpd                     ┆ 18067      ┆ 5.88         │\n",
      "│ pos_max_dpd_def                 ┆ 18067      ┆ 5.88         │\n",
      "│ pos_completed_rate              ┆ 18067      ┆ 5.88         │\n",
      "│ pos_active_rate                 ┆ 18067      ┆ 5.88         │\n",
      "│ days_since_last_approval_prev   ┆ 17446      ┆ 5.67         │\n",
      "│ avg_term_approved_prev          ┆ 17446      ┆ 5.67         │\n",
      "│ max_prev_annuity_prev           ┆ 16871      ┆ 5.49         │\n",
      "│ application_count_prev          ┆ 16454      ┆ 5.35         │\n",
      "│ refusal_rate_prev               ┆ 16454      ┆ 5.35         │\n",
      "│ approval_rate_prev              ┆ 16454      ┆ 5.35         │\n",
      "│ cash_loan_rate_prev             ┆ 16454      ┆ 5.35         │\n",
      "│ walk_in_rate_prev               ┆ 16454      ┆ 5.35         │\n",
      "│ insurance_uptake_rate_prev      ┆ 16454      ┆ 5.35         │\n",
      "│ high_yield_group_rate_prev      ┆ 16454      ┆ 5.35         │\n",
      "│ most_freq_seller_industry_prev  ┆ 16454      ┆ 5.35         │\n",
      "│ ins_late_payment_count          ┆ 15868      ┆ 5.16         │\n",
      "│ ins_avg_dpd                     ┆ 15868      ┆ 5.16         │\n",
      "│ ins_max_dpd                     ┆ 15868      ┆ 5.16         │\n",
      "│ ins_avg_dbd                     ┆ 15868      ┆ 5.16         │\n",
      "│ ins_max_dbd                     ┆ 15868      ┆ 5.16         │\n",
      "│ ins_underpayment_count          ┆ 15868      ┆ 5.16         │\n",
      "│ ins_payment_ratio               ┆ 15879      ┆ 5.16         │\n",
      "│ ins_total_missed_balance        ┆ 15876      ┆ 5.16         │\n",
      "│ ins_version_change_count        ┆ 15868      ┆ 5.16         │\n",
      "│ ins_recent_late_count_1y        ┆ 15868      ┆ 5.16         │\n",
      "│ NAME_TYPE_SUITE                 ┆ 1292       ┆ 0.42         │\n",
      "│ OBS_30_CNT_SOCIAL_CIRCLE        ┆ 1021       ┆ 0.33         │\n",
      "│ DEF_30_CNT_SOCIAL_CIRCLE        ┆ 1021       ┆ 0.33         │\n",
      "│ OBS_60_CNT_SOCIAL_CIRCLE        ┆ 1021       ┆ 0.33         │\n",
      "│ DEF_60_CNT_SOCIAL_CIRCLE        ┆ 1021       ┆ 0.33         │\n",
      "│ EXT_SOURCE_2                    ┆ 660        ┆ 0.21         │\n",
      "│ AMT_GOODS_PRICE                 ┆ 278        ┆ 0.09         │\n",
      "│ AMT_ANNUITY                     ┆ 12         ┆ 0.0          │\n",
      "│ CNT_FAM_MEMBERS                 ┆ 2          ┆ 0.0          │\n",
      "│ DAYS_LAST_PHONE_CHANGE          ┆ 1          ┆ 0.0          │\n",
      "│ ratio_annuity_to_income         ┆ 12         ┆ 0.0          │\n",
      "│ ratio_annuity_to_credit         ┆ 12         ┆ 0.0          │\n",
      "│ feature_disposable_income       ┆ 12         ┆ 0.0          │\n",
      "└─────────────────────────────────┴────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate Null Stats\n",
    "# We compute null counts for all columns at once\n",
    "null_stats = df.select([\n",
    "    pl.col(c).null_count().alias(c) for c in df.columns\n",
    "])\n",
    "\n",
    "# 2. Transpose for Readability\n",
    "# Switches from wide (many columns) to long (Feature, Count) format\n",
    "null_df = null_stats.transpose(\n",
    "    include_header=True, \n",
    "    header_name=\"Feature\", \n",
    "    column_names=[\"Null_Count\"]\n",
    ")\n",
    "\n",
    "# 3. Calculate Percentage and Formatting\n",
    "total_rows = df.height\n",
    "\n",
    "null_df = (\n",
    "    null_df\n",
    "    .with_columns(\n",
    "        (pl.col(\"Null_Count\") / total_rows * 100).round(2).alias(\"Null_Percent\")\n",
    "    )\n",
    "    .filter(pl.col(\"Null_Count\") > 0)  # Optional: Hide columns with 0 nulls\n",
    "    .sort(\"Null_Percent\", descending=True)\n",
    ")\n",
    "\n",
    "# 4. Display Results\n",
    "print(f\"\\n--- Missing Value Report (Total Rows: {total_rows}) ---\")\n",
    "print(f\"Columns with missing values: {null_df.height}\")\n",
    "\n",
    "# Show all rows of the report\n",
    "with pl.Config(tbl_rows=-1):\n",
    "    print(null_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Missing columns handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:38:41.964404Z",
     "iopub.status.busy": "2025-12-27T19:38:41.964049Z",
     "iopub.status.idle": "2025-12-27T19:38:41.975603Z",
     "shell.execute_reply": "2025-12-27T19:38:41.974933Z",
     "shell.execute_reply.started": "2025-12-27T19:38:41.964377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing columns with > 90.0% missing values...\n",
      "Found 1 columns to drop:\n",
      "['avg_rate_interest_approved_prev']\n",
      "\n",
      "Drop complete.\n",
      "New DataFrame Shape: (307511, 127)\n"
     ]
    }
   ],
   "source": [
    "# 1. Define Threshold\n",
    "limit = 90.0\n",
    "total_rows = df.height\n",
    "\n",
    "print(f\"Analyzing columns with > {limit}% missing values...\")\n",
    "\n",
    "# 2. Calculate Null Percentages\n",
    "# We check every column and calculate its missing percentage\n",
    "null_stats = df.select([\n",
    "    (pl.col(c).null_count() / total_rows * 100).alias(c) for c in df.columns\n",
    "])\n",
    "\n",
    "# 3. Identify Columns to Drop\n",
    "drop_cols_high_null = [\n",
    "    c for c in null_stats.columns if null_stats[c][0] > limit\n",
    "]\n",
    "\n",
    "# 4. Execute Drop\n",
    "if drop_cols_high_null:\n",
    "    print(f\"Found {len(drop_cols_high_null)} columns to drop:\")\n",
    "    print(drop_cols_high_null)\n",
    "    \n",
    "    df = df.drop(drop_cols_high_null)\n",
    "    print(\"\\nDrop complete.\")\n",
    "else:\n",
    "    print(\"\\nNo columns found above the 90% missing threshold.\")\n",
    "\n",
    "# 5. Verify Status\n",
    "print(f\"New DataFrame Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output indicates that the dataset is relatively dense, as only **one feature** exceeded the 90% missingness threshold: `avg_rate_interest_approved_prev`.\n",
    "\n",
    "* **Sparsity Removal:** We dropped this specific column because a feature missing 90% of its data contains almost no statistical signal. Imputing values for such a large portion of the rows would introduce significant bias and artificial noise into the model.\n",
    "* **Dimensionality Update:** The dataframe shape has been updated to **127 columns**. This confirms the drop was executed correctly.\n",
    "* **Data Quality:** The fact that only one column was removed suggests that the remaining features have sufficient data density to be useful for modeling, though they may still require standard imputation techniques later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Categorical Cardinality Check**\n",
    "\n",
    "**Aim**\n",
    "Before we convert our text columns (categorical variables) into numbers for the machine learning model, we must assess their \"Cardinality\"—the number of unique values in each column.\n",
    "\n",
    "This step is crucial for choosing the correct encoding strategy:\n",
    "\n",
    "* **Low Cardinality (e.g., Gender: M/F):** These are simple to handle. We can use Label Encoding or One-Hot Encoding without increasing the dataset size significantly.\n",
    "* **High Cardinality (e.g., Organization Type: 50+ job types):** These are dangerous. If we use standard One-Hot Encoding on a column with 100 categories, we add 100 new columns to the dataset. This causes the \"Curse of Dimensionality,\" where the data becomes sparse and the model struggles to find patterns.\n",
    "\n",
    "This code scans all string/object columns, counts their unique values, and flags any feature with more than 50 categories. This \"High Cardinality Alert\" tells us which columns require advanced techniques like **Weight of Evidence (WoE)** or **Target Encoding** instead of simple labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:38:53.290127Z",
     "iopub.status.busy": "2025-12-27T19:38:53.289589Z",
     "iopub.status.idle": "2025-12-27T19:38:53.330420Z",
     "shell.execute_reply": "2025-12-27T19:38:53.329780Z",
     "shell.execute_reply.started": "2025-12-27T19:38:53.290096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 16 categorical columns...\n",
      "\n",
      "--- Categorical Cardinality Report ---\n",
      "shape: (16, 2)\n",
      "┌────────────────────────────────┬──────────────┐\n",
      "│ Feature                        ┆ Unique_Count │\n",
      "│ ---                            ┆ ---          │\n",
      "│ str                            ┆ u32          │\n",
      "╞════════════════════════════════╪══════════════╡\n",
      "│ ORGANIZATION_TYPE              ┆ 58           │\n",
      "│ OCCUPATION_TYPE                ┆ 19           │\n",
      "│ most_freq_seller_industry_prev ┆ 12           │\n",
      "│ NAME_TYPE_SUITE                ┆ 8            │\n",
      "│ NAME_INCOME_TYPE               ┆ 8            │\n",
      "│ WALLSMATERIAL_MODE             ┆ 8            │\n",
      "│ WEEKDAY_APPR_PROCESS_START     ┆ 7            │\n",
      "│ NAME_FAMILY_STATUS             ┆ 6            │\n",
      "│ NAME_HOUSING_TYPE              ┆ 6            │\n",
      "│ NAME_EDUCATION_TYPE            ┆ 5            │\n",
      "│ FONDKAPREMONT_MODE             ┆ 5            │\n",
      "│ HOUSETYPE_MODE                 ┆ 4            │\n",
      "│ CODE_GENDER                    ┆ 3            │\n",
      "│ NAME_CONTRACT_TYPE             ┆ 2            │\n",
      "│ FLAG_OWN_CAR                   ┆ 2            │\n",
      "│ FLAG_OWN_REALTY                ┆ 2            │\n",
      "└────────────────────────────────┴──────────────┘\n",
      "\n",
      " ALERT: 1 columns have high cardinality (> 50 unique values).\n",
      "We should group rare categories for these.\n"
     ]
    }
   ],
   "source": [
    "# 1. Identify Categorical Columns\n",
    "# We look for String, Object, or Categorical types\n",
    "cat_cols = [\n",
    "    c for c, t in df.schema.items() \n",
    "    if t in (pl.String, pl.Object, pl.Categorical)\n",
    "]\n",
    "\n",
    "print(f\"Analyzing {len(cat_cols)} categorical columns...\")\n",
    "\n",
    "if len(cat_cols) > 0:\n",
    "    # 2. Calculate Unique Counts (Cardinality)\n",
    "    cardinality_df = df.select([\n",
    "        pl.col(c).n_unique().alias(c) for c in cat_cols\n",
    "    ]).transpose(\n",
    "        include_header=True, \n",
    "        header_name=\"Feature\", \n",
    "        column_names=[\"Unique_Count\"]\n",
    "    ).sort(\"Unique_Count\", descending=True)\n",
    "\n",
    "    # 3. Display Results\n",
    "    print(f\"\\n--- Categorical Cardinality Report ---\")\n",
    "    with pl.Config(tbl_rows=-1):\n",
    "        print(cardinality_df)\n",
    "        \n",
    "    # 4. Highlight High Cardinality Suspects\n",
    "    # A count > 50 usually requires \"Rare Label Encoding\" (grouping small categories into \"OTHER\")\n",
    "    high_card_cols = cardinality_df.filter(pl.col(\"Unique_Count\") > 50)\n",
    "    \n",
    "    if high_card_cols.height > 0:\n",
    "        print(f\"\\n ALERT: {high_card_cols.height} columns have high cardinality (> 50 unique values).\")\n",
    "        print(\"We should group rare categories for these.\")\n",
    "else:\n",
    "    print(\"No categorical columns found. (Did you already encode them?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cardinality analysis provides a clear roadmap for our encoding strategy.\n",
    "\n",
    "**1. High Cardinality Identified (`ORGANIZATION_TYPE`)**\n",
    "As suspected, `ORGANIZATION_TYPE` is the outlier with **58 unique values**.\n",
    "\n",
    "* **Implication:** If we were to use standard One-Hot Encoding here, it would add 58 new columns to our dataset, significantly increasing dimensionality and computational cost.\n",
    "* **Action:** This confirms that `ORGANIZATION_TYPE` is the primary candidate for **Weight of Evidence (WoE)** encoding. This technique will convert these 58 categories into a single numerical risk score, preserving the information without expanding the dataset.\n",
    "\n",
    "**2. Moderate Cardinality (`OCCUPATION_TYPE`, `most_freq_seller_industry_prev`)**\n",
    "These features have 19 and 12 unique values respectively. While not strictly \"high\" cardinality (usually defined as >50 or >100), they are complex enough that they might benefit from WoE encoding as well, rather than simple Label Encoding which imposes an arbitrary order (e.g., Accountants < Drivers).\n",
    "\n",
    "**3. Low Cardinality (The Safe Zone)**\n",
    "The remaining 13 features (like `CODE_GENDER`, `NAME_FAMILY_STATUS`, `FLAG_OWN_CAR`) have fewer than 10 unique values. These are computationally inexpensive and can be handled safely with standard Label Encoding or binary mapping (0 and 1).\n",
    "\n",
    "**Next Step:**\n",
    "We will now proceed to apply the **Weight of Evidence (WoE)** transformation, specifically targeting the high-cardinality `ORGANIZATION_TYPE` to mathematically capture the default risk associated with different employers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analyzing Organization Type Distribution**\n",
    "\n",
    "**Aim:**\n",
    "We identified `ORGANIZATION_TYPE` as our highest cardinality feature (58 categories). Before we apply advanced mathematical transformations like Weight of Evidence (WoE), we must understand its distribution.\n",
    "\n",
    "My objective here is to inspect the **frequency** of each job type.\n",
    "\n",
    "* **Dominant Categories:** Which industries make up the majority of our applicants?\n",
    "* **The \"Long Tail\" of Rare Labels:** Are there job types with very few samples (e.g., \"Industry: Type 13\" having only 10 people)?\n",
    "\n",
    "This distinction is critical. Calculating a default risk score for a category with only 10 people is statistically dangerous—one default would swing the score wildly (high variance). If we find many rare categories, we will know that our WoE calculation needs \"smoothing\" to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:42:13.413200Z",
     "iopub.status.busy": "2025-12-27T19:42:13.412593Z",
     "iopub.status.idle": "2025-12-27T19:42:13.503693Z",
     "shell.execute_reply": "2025-12-27T19:42:13.502767Z",
     "shell.execute_reply.started": "2025-12-27T19:42:13.413171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ORGANIZATION_TYPE Distribution (58 unique values) ---\n",
      "shape: (58, 3)\n",
      "┌────────────────────────┬───────┬──────────────────┐\n",
      "│ ORGANIZATION_TYPE      ┆ len   ┆ Percent_of_Total │\n",
      "│ ---                    ┆ ---   ┆ ---              │\n",
      "│ str                    ┆ u32   ┆ f64              │\n",
      "╞════════════════════════╪═══════╪══════════════════╡\n",
      "│ Business Entity Type 3 ┆ 67992 ┆ 22.11            │\n",
      "│ XNA                    ┆ 55374 ┆ 18.01            │\n",
      "│ Self-employed          ┆ 38412 ┆ 12.49            │\n",
      "│ Other                  ┆ 16683 ┆ 5.43             │\n",
      "│ Medicine               ┆ 11193 ┆ 3.64             │\n",
      "│ Business Entity Type 2 ┆ 10553 ┆ 3.43             │\n",
      "│ Government             ┆ 10404 ┆ 3.38             │\n",
      "│ School                 ┆ 8893  ┆ 2.89             │\n",
      "│ Trade: type 7          ┆ 7831  ┆ 2.55             │\n",
      "│ Kindergarten           ┆ 6880  ┆ 2.24             │\n",
      "│ Construction           ┆ 6721  ┆ 2.19             │\n",
      "│ Business Entity Type 1 ┆ 5984  ┆ 1.95             │\n",
      "│ Transport: type 4      ┆ 5398  ┆ 1.76             │\n",
      "│ Trade: type 3          ┆ 3492  ┆ 1.14             │\n",
      "│ Industry: type 9       ┆ 3368  ┆ 1.1              │\n",
      "│ Industry: type 3       ┆ 3278  ┆ 1.07             │\n",
      "│ Security               ┆ 3247  ┆ 1.06             │\n",
      "│ Housing                ┆ 2958  ┆ 0.96             │\n",
      "│ Industry: type 11      ┆ 2704  ┆ 0.88             │\n",
      "│ Military               ┆ 2634  ┆ 0.86             │\n",
      "│ Bank                   ┆ 2507  ┆ 0.82             │\n",
      "│ Agriculture            ┆ 2454  ┆ 0.8              │\n",
      "│ Police                 ┆ 2341  ┆ 0.76             │\n",
      "│ Transport: type 2      ┆ 2204  ┆ 0.72             │\n",
      "│ Postal                 ┆ 2157  ┆ 0.7              │\n",
      "│ Security Ministries    ┆ 1974  ┆ 0.64             │\n",
      "│ Trade: type 2          ┆ 1900  ┆ 0.62             │\n",
      "│ Restaurant             ┆ 1811  ┆ 0.59             │\n",
      "│ Services               ┆ 1575  ┆ 0.51             │\n",
      "│ University             ┆ 1327  ┆ 0.43             │\n",
      "│ Industry: type 7       ┆ 1307  ┆ 0.43             │\n",
      "│ Transport: type 3      ┆ 1187  ┆ 0.39             │\n",
      "│ Industry: type 1       ┆ 1039  ┆ 0.34             │\n",
      "│ Hotel                  ┆ 966   ┆ 0.31             │\n",
      "│ Electricity            ┆ 950   ┆ 0.31             │\n",
      "│ Industry: type 4       ┆ 877   ┆ 0.29             │\n",
      "│ Trade: type 6          ┆ 631   ┆ 0.21             │\n",
      "│ Industry: type 5       ┆ 599   ┆ 0.19             │\n",
      "│ Insurance              ┆ 597   ┆ 0.19             │\n",
      "│ Telecom                ┆ 577   ┆ 0.19             │\n",
      "│ Emergency              ┆ 560   ┆ 0.18             │\n",
      "│ Industry: type 2       ┆ 458   ┆ 0.15             │\n",
      "│ Advertising            ┆ 429   ┆ 0.14             │\n",
      "│ Realtor                ┆ 396   ┆ 0.13             │\n",
      "│ Culture                ┆ 379   ┆ 0.12             │\n",
      "│ Industry: type 12      ┆ 369   ┆ 0.12             │\n",
      "│ Trade: type 1          ┆ 348   ┆ 0.11             │\n",
      "│ Mobile                 ┆ 317   ┆ 0.1              │\n",
      "│ Legal Services         ┆ 305   ┆ 0.1              │\n",
      "│ Cleaning               ┆ 260   ┆ 0.08             │\n",
      "│ Transport: type 1      ┆ 201   ┆ 0.07             │\n",
      "│ Industry: type 6       ┆ 112   ┆ 0.04             │\n",
      "│ Industry: type 10      ┆ 109   ┆ 0.04             │\n",
      "│ Religion               ┆ 85    ┆ 0.03             │\n",
      "│ Industry: type 13      ┆ 67    ┆ 0.02             │\n",
      "│ Trade: type 4          ┆ 64    ┆ 0.02             │\n",
      "│ Trade: type 5          ┆ 49    ┆ 0.02             │\n",
      "│ Industry: type 8       ┆ 24    ┆ 0.01             │\n",
      "└────────────────────────┴───────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate Counts for ORGANIZATION_TYPE\n",
    "# We use .len() instead of .count()\n",
    "org_counts = (\n",
    "    df.group_by(\"ORGANIZATION_TYPE\")\n",
    "    .len()\n",
    "    .sort(\"len\", descending=True)\n",
    ")\n",
    "\n",
    "# 2. Add Percentage for context\n",
    "org_counts = org_counts.with_columns(\n",
    "    (pl.col(\"len\") / df.height * 100).round(2).alias(\"Percent_of_Total\")\n",
    ")\n",
    "\n",
    "# 3. Display the Full Table\n",
    "print(f\"\\n--- ORGANIZATION_TYPE Distribution ({org_counts.height} unique values) ---\")\n",
    "\n",
    "with pl.Config(tbl_rows=-1):\n",
    "    print(org_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analyzing Risk per Organization Type**\n",
    "\n",
    "**Aim:**\n",
    "Previous steps told us *how many* people work in each industry; this step tells us *how risky* they are. This is a classic **Bivariate Analysis** .\n",
    "\n",
    "We group the data by `ORGANIZATION_TYPE` and calculate the mean of the `TARGET` variable. Since our target is binary (0 = Repaid, 1 = Defaulted), the mean represents the **Default Probability**.\n",
    "\n",
    "* If the mean is `0.05`, it implies a **5% Default Rate** for that specific group.\n",
    "\n",
    "We also calculate the **Global Average Default Rate** to use as a baseline.\n",
    "\n",
    "* Industries **below** this line are \"Safe\" (protective factors).\n",
    "* Industries **above** this line are \"Risky\" (risk factors).\n",
    "\n",
    "By sorting the table from lowest to highest, we create a clear \"Risk Hierarchy.\" This validates the feature's predictive power: if all industries had the same default rate, the feature would be useless. We expect to see a wide spread (variance) in risk between sectors like \"Police\" (stable) vs \"Restaurant\" (volatile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:42:24.872069Z",
     "iopub.status.busy": "2025-12-27T19:42:24.871522Z",
     "iopub.status.idle": "2025-12-27T19:42:24.947678Z",
     "shell.execute_reply": "2025-12-27T19:42:24.946901Z",
     "shell.execute_reply.started": "2025-12-27T19:42:24.872039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Organization Types Sorted by Risk (Target Mean) ---\n",
      "Global Average Default Rate: 8.07%\n",
      "shape: (58, 4)\n",
      "┌────────────────────────┬───────┬──────────────┬────────────────┐\n",
      "│ ORGANIZATION_TYPE      ┆ Count ┆ Default_Rate ┆ Default_Rate_% │\n",
      "│ ---                    ┆ ---   ┆ ---          ┆ ---            │\n",
      "│ str                    ┆ u32   ┆ f64          ┆ f64            │\n",
      "╞════════════════════════╪═══════╪══════════════╪════════════════╡\n",
      "│ Trade: type 4          ┆ 64    ┆ 0.03125      ┆ 3.13           │\n",
      "│ Industry: type 12      ┆ 369   ┆ 0.03794      ┆ 3.79           │\n",
      "│ Transport: type 1      ┆ 201   ┆ 0.044776     ┆ 4.48           │\n",
      "│ Trade: type 6          ┆ 631   ┆ 0.045959     ┆ 4.6            │\n",
      "│ Security Ministries    ┆ 1974  ┆ 0.048632     ┆ 4.86           │\n",
      "│ University             ┆ 1327  ┆ 0.048983     ┆ 4.9            │\n",
      "│ Police                 ┆ 2341  ┆ 0.049979     ┆ 5.0            │\n",
      "│ Military               ┆ 2634  ┆ 0.051253     ┆ 5.13           │\n",
      "│ Bank                   ┆ 2507  ┆ 0.051855     ┆ 5.19           │\n",
      "│ XNA                    ┆ 55374 ┆ 0.053996     ┆ 5.4            │\n",
      "│ Culture                ┆ 379   ┆ 0.055409     ┆ 5.54           │\n",
      "│ Insurance              ┆ 597   ┆ 0.056951     ┆ 5.7            │\n",
      "│ Religion               ┆ 85    ┆ 0.058824     ┆ 5.88           │\n",
      "│ School                 ┆ 8893  ┆ 0.059148     ┆ 5.91           │\n",
      "│ Trade: type 5          ┆ 49    ┆ 0.061224     ┆ 6.12           │\n",
      "│ Hotel                  ┆ 966   ┆ 0.064182     ┆ 6.42           │\n",
      "│ Industry: type 10      ┆ 109   ┆ 0.06422      ┆ 6.42           │\n",
      "│ Medicine               ┆ 11193 ┆ 0.065845     ┆ 6.58           │\n",
      "│ Services               ┆ 1575  ┆ 0.066032     ┆ 6.6            │\n",
      "│ Electricity            ┆ 950   ┆ 0.066316     ┆ 6.63           │\n",
      "│ Industry: type 9       ┆ 3368  ┆ 0.066805     ┆ 6.68           │\n",
      "│ Industry: type 5       ┆ 599   ┆ 0.068447     ┆ 6.84           │\n",
      "│ Government             ┆ 10404 ┆ 0.069781     ┆ 6.98           │\n",
      "│ Trade: type 2          ┆ 1900  ┆ 0.07         ┆ 7.0            │\n",
      "│ Kindergarten           ┆ 6880  ┆ 0.070349     ┆ 7.03           │\n",
      "│ Industry: type 6       ┆ 112   ┆ 0.071429     ┆ 7.14           │\n",
      "│ Emergency              ┆ 560   ┆ 0.071429     ┆ 7.14           │\n",
      "│ Industry: type 2       ┆ 458   ┆ 0.072052     ┆ 7.21           │\n",
      "│ Telecom                ┆ 577   ┆ 0.076256     ┆ 7.63           │\n",
      "│ Other                  ┆ 16683 ┆ 0.076425     ┆ 7.64           │\n",
      "│ Transport: type 2      ┆ 2204  ┆ 0.07804      ┆ 7.8            │\n",
      "│ Legal Services         ┆ 305   ┆ 0.078689     ┆ 7.87           │\n",
      "│ Housing                ┆ 2958  ┆ 0.079446     ┆ 7.94           │\n",
      "│ Industry: type 7       ┆ 1307  ┆ 0.080337     ┆ 8.03           │\n",
      "│ Business Entity Type 1 ┆ 5984  ┆ 0.081384     ┆ 8.14           │\n",
      "│ Advertising            ┆ 429   ┆ 0.081585     ┆ 8.16           │\n",
      "│ Postal                 ┆ 2157  ┆ 0.084376     ┆ 8.44           │\n",
      "│ Business Entity Type 2 ┆ 10553 ┆ 0.085284     ┆ 8.53           │\n",
      "│ Industry: type 11      ┆ 2704  ┆ 0.086538     ┆ 8.65           │\n",
      "│ Trade: type 1          ┆ 348   ┆ 0.08908      ┆ 8.91           │\n",
      "│ Mobile                 ┆ 317   ┆ 0.091483     ┆ 9.15           │\n",
      "│ Transport: type 4      ┆ 5398  ┆ 0.092812     ┆ 9.28           │\n",
      "│ Business Entity Type 3 ┆ 67992 ┆ 0.092996     ┆ 9.3            │\n",
      "│ Trade: type 7          ┆ 7831  ┆ 0.094496     ┆ 9.45           │\n",
      "│ Security               ┆ 3247  ┆ 0.099784     ┆ 9.98           │\n",
      "│ Industry: type 4       ┆ 877   ┆ 0.101482     ┆ 10.15          │\n",
      "│ Self-employed          ┆ 38412 ┆ 0.101739     ┆ 10.17          │\n",
      "│ Trade: type 3          ┆ 3492  ┆ 0.103379     ┆ 10.34          │\n",
      "│ Agriculture            ┆ 2454  ┆ 0.104727     ┆ 10.47          │\n",
      "│ Realtor                ┆ 396   ┆ 0.106061     ┆ 10.61          │\n",
      "│ Industry: type 3       ┆ 3278  ┆ 0.106162     ┆ 10.62          │\n",
      "│ Industry: type 1       ┆ 1039  ┆ 0.110683     ┆ 11.07          │\n",
      "│ Cleaning               ┆ 260   ┆ 0.111538     ┆ 11.15          │\n",
      "│ Construction           ┆ 6721  ┆ 0.116798     ┆ 11.68          │\n",
      "│ Restaurant             ┆ 1811  ┆ 0.117062     ┆ 11.71          │\n",
      "│ Industry: type 8       ┆ 24    ┆ 0.125        ┆ 12.5           │\n",
      "│ Industry: type 13      ┆ 67    ┆ 0.134328     ┆ 13.43          │\n",
      "│ Transport: type 3      ┆ 1187  ┆ 0.15754      ┆ 15.75          │\n",
      "└────────────────────────┴───────┴──────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate Default Rate for each Organization\n",
    "risk_stats = (\n",
    "    df.group_by(\"ORGANIZATION_TYPE\")\n",
    "    .agg([\n",
    "        pl.len().alias(\"Count\"),\n",
    "        pl.col(\"TARGET\").mean().alias(\"Default_Rate\")\n",
    "    ])\n",
    "    .with_columns(\n",
    "        (pl.col(\"Default_Rate\") * 100).round(2).alias(\"Default_Rate_%\")\n",
    "    )\n",
    "    .sort(\"Default_Rate\") # Sort from Safest to Riskiest\n",
    ")\n",
    "\n",
    "# 2. Display the Full Table\n",
    "print(f\"\\n--- Organization Types Sorted by Risk (Target Mean) ---\")\n",
    "print(f\"Global Average Default Rate: {(df['TARGET'].mean() * 100):.2f}%\")\n",
    "\n",
    "with pl.Config(tbl_rows=-1):\n",
    "    print(risk_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The risk stratification table confirms that `ORGANIZATION_TYPE` is a powerful predictor. We see a massive spread in default rates, ranging from **3%** (safest) to nearly **16%** (riskiest). This variance is exactly what a machine learning model needs to discriminate between good and bad borrowers.\n",
    "\n",
    "**1. The \"Safe Havens\" (State & Stability)**\n",
    "The safest borrowers are consistently found in stable, often state-funded sectors.\n",
    "\n",
    "* **Security Ministries (4.86%), Police (5.0%), Military (5.13%):** These default rates are nearly **half** of the global average (8.07%).\n",
    "* **Bank (5.19%) & University (4.9%):** White-collar professionals in established institutions also show excellent repayment behavior.\n",
    "* **XNA (5.4%):** This confirms our earlier hypothesis. The 55,000 \"XNA\" applicants (likely pensioners) are among the safest groups. Their fixed pension income makes them reliable borrowers compared to the active workforce.\n",
    "\n",
    "**2. The \"Danger Zones\" (Volatility & Labor)**\n",
    "On the other end of the spectrum, we see industries characterized by income volatility or manual labor.\n",
    "\n",
    "* **Transport: type 3 (15.75%):** This is the single riskiest category, with a default rate **double** the global average.\n",
    "* **Construction (11.68%) & Restaurant (11.71%):** These sectors are economically sensitive. A construction worker or waiter is more likely to face irregular income, leading to higher default rates.\n",
    "* **Self-employed (10.17%):** With a massive sample size of 38,000, this is a significant finding. Self-employed individuals are statistically **25% riskier** than the average applicant.\n",
    "\n",
    "**3. The Statistical Trap (Why we need WoE)**\n",
    "Look at the very top and bottom rows:\n",
    "\n",
    "* **Safest:** `Trade: type 4` (3.13%) — *But only 64 people.*\n",
    "* **Riskiest:** `Industry: type 8` (12.5%) — *But only 24 people.*\n",
    "\n",
    "These extreme values are likely statistical flukes caused by small sample sizes. If we fed these raw numbers to a model, it would overreact. This perfectly justifies our next step: **Weight of Evidence (WoE) with Smoothing**. The smoothing will pull these \"tiny\" categories closer to the global average (8.07%), preventing the model from making extreme decisions based on just 24 people.\n",
    "\n",
    "**Next Step:**\n",
    "Now that we have proven the risk varies by industry (and identified the small-sample danger), we will mathematically encode this using the smoothed WoE formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Weight of Evidence (WoE) Calculation for Organization Type**\n",
    "\n",
    "We have established that `ORGANIZATION_TYPE` is a high-cardinality feature (58 categories) with a strong correlation to default risk. However, machine learning models cannot understand text strings like \"Police\" or \"Construction.\" Our aim here is to convert these qualitative labels into quantitative \"Risk Scores\" without exploding the dataset size (which One-Hot Encoding would do).\n",
    "\n",
    "To do this, we are applying a statistical technique called **Weight of Evidence (WoE)**.\n",
    "\n",
    "This transformation aims to:\n",
    "\n",
    "1. **Quantify Risk:** Convert categories into a continuous number where a **positive value** indicates a \"Safe\" job (higher probability of repayment) and a **negative value** indicates a \"Risky\" job (higher probability of default).\n",
    "2. **Handle Rare Categories:** By using **Laplace Smoothing** (adding `+0.5` to our counts), we prevent mathematical errors (division by zero) and stabilize the scores for those tiny industries we found earlier (like `Industry: type 8` with only 24 people). This ensures our model doesn't panic and overfit to small sample sizes.\n",
    "3. **Linearize the Signal:** WoE scales the data in a way that makes it easier for linear models and gradient boosting trees to distinguish between good and bad borrowers efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:42:49.236899Z",
     "iopub.status.busy": "2025-12-27T19:42:49.236577Z",
     "iopub.status.idle": "2025-12-27T19:42:49.416702Z",
     "shell.execute_reply": "2025-12-27T19:42:49.415909Z",
     "shell.execute_reply.started": "2025-12-27T19:42:49.236875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Weight of Evidence (WoE) for Organization Type...\n",
      "\n",
      "WoE Transformation Complete.\n",
      "Top 5 Safest Jobs (High WoE):\n",
      "shape: (5, 2)\n",
      "┌─────────────────────┬───────────────────────┐\n",
      "│ ORGANIZATION_TYPE   ┆ ORGANIZATION_TYPE_WoE │\n",
      "│ ---                 ┆ ---                   │\n",
      "│ str                 ┆ f64                   │\n",
      "╞═════════════════════╪═══════════════════════╡\n",
      "│ Trade: type 4       ┆ 0.78639               │\n",
      "│ Industry: type 12   ┆ 0.766891              │\n",
      "│ Trade: type 6       ┆ 0.584212              │\n",
      "│ Transport: type 1   ┆ 0.576319              │\n",
      "│ Security Ministries ┆ 0.5362                │\n",
      "└─────────────────────┴───────────────────────┘\n",
      "\n",
      "Top 5 Riskiest Jobs (Low WoE):\n",
      "shape: (5, 2)\n",
      "┌───────────────────┬───────────────────────┐\n",
      "│ ORGANIZATION_TYPE ┆ ORGANIZATION_TYPE_WoE │\n",
      "│ ---               ┆ ---                   │\n",
      "│ str               ┆ f64                   │\n",
      "╞═══════════════════╪═══════════════════════╡\n",
      "│ Transport: type 3 ┆ -0.758009             │\n",
      "│ Industry: type 8  ┆ -0.617196             │\n",
      "│ Industry: type 13 ┆ -0.614751             │\n",
      "│ Restaurant        ┆ -0.413981             │\n",
      "│ Construction      ┆ -0.409931             │\n",
      "└───────────────────┴───────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Weight of Evidence (WoE) for Organization Type...\")\n",
    "\n",
    "# --- 1. SAFE RESET (Fixes the Duplicate Error) ---\n",
    "# If we already created this column, drop it so we can start fresh.\n",
    "if \"ORGANIZATION_TYPE_WoE\" in df.columns:\n",
    "    print(\"   Removing existing WoE column to avoid duplicates...\")\n",
    "    df = df.drop(\"ORGANIZATION_TYPE_WoE\")\n",
    "\n",
    "# --- 2. CALCULATE GLOBAL COUNTS ---\n",
    "total_goods = df.filter(pl.col(\"TARGET\") == 0).height\n",
    "total_bads = df.filter(pl.col(\"TARGET\") == 1).height\n",
    "\n",
    "# --- 3. CALCULATE WoE TABLE ---\n",
    "woe_df = (\n",
    "    df.group_by(\"ORGANIZATION_TYPE\")\n",
    "    .agg([\n",
    "        pl.len().alias(\"n_total\"),\n",
    "        pl.col(\"TARGET\").sum().alias(\"n_bad\"),\n",
    "        (pl.len() - pl.col(\"TARGET\").sum()).alias(\"n_good\")\n",
    "    ])\n",
    "    .filter(pl.col(\"n_total\") > 0)\n",
    ")\n",
    "\n",
    "# Apply Laplace Smoothing (+0.5) to handle zeros\n",
    "woe_df = woe_df.with_columns(\n",
    "    (pl.col(\"n_good\") + 0.5).alias(\"n_good_smooth\"),\n",
    "    (pl.col(\"n_bad\") + 0.5).alias(\"n_bad_smooth\")\n",
    ")\n",
    "\n",
    "# Calculate WoE Log-Odds\n",
    "woe_df = woe_df.with_columns(\n",
    "    (\n",
    "        np.log(\n",
    "            (pl.col(\"n_good_smooth\") / total_goods) / \n",
    "            (pl.col(\"n_bad_smooth\") / total_bads)\n",
    "        )\n",
    "    ).alias(\"ORGANIZATION_TYPE_WoE\")\n",
    ")\n",
    "\n",
    "# --- 4. MERGE BACK SAFELY ---\n",
    "df = df.join(\n",
    "    woe_df.select([\"ORGANIZATION_TYPE\", \"ORGANIZATION_TYPE_WoE\"]), \n",
    "    on=\"ORGANIZATION_TYPE\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# --- 5. CHECK RESULTS ---\n",
    "print(\"\\nWoE Transformation Complete.\")\n",
    "print(\"Top 5 Safest Jobs (High WoE):\")\n",
    "print(df.select([\"ORGANIZATION_TYPE\", \"ORGANIZATION_TYPE_WoE\"]).unique().sort(\"ORGANIZATION_TYPE_WoE\", descending=True).head(5))\n",
    "\n",
    "print(\"\\nTop 5 Riskiest Jobs (Low WoE):\")\n",
    "print(df.select([\"ORGANIZATION_TYPE\", \"ORGANIZATION_TYPE_WoE\"]).unique().sort(\"ORGANIZATION_TYPE_WoE\", descending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WoE transformation has successfully converted our qualitative job labels into a quantitative \"Risk Score.\" The results provide a clear mathematical validation of our risk hypothesis.\n",
    "\n",
    "**1. The \"Scale of Safety\" (Understanding the Score)**\n",
    "The WoE values center around **0.0**.\n",
    "\n",
    "* **Positive values (> 0)** indicate \"Good\" borrowers (lower default rates than the population average).\n",
    "* **Negative values (< 0)** indicate \"Bad\" borrowers (higher default rates).\n",
    "* The magnitude tells us the strength. A score of `+0.78` is a massive indicator of safety, while `-0.75` is a screaming red flag for risk.\n",
    "\n",
    "**2. Validation of Safe Jobs (Positive WoE)**\n",
    "\n",
    "* **`Trade: type 4` (+0.78) & `Industry: type 12` (+0.76):** These categories received the highest scores, meaning applicants from these fields are statistically the least likely to default.\n",
    "* **`Security Ministries` (+0.53):** This is the most important finding here. Unlike the smaller \"Trade\" categories, this is a large, stable government sector. Its strong positive score confirms that state employment acts as a massive \"protective factor\" against default.\n",
    "\n",
    "**3. Validation of Risky Jobs (Negative WoE)**\n",
    "\n",
    "* **`Transport: type 3` (-0.75):** This is the single strongest risk indicator in the entire feature. An applicant listing this job is heavily penalized by the model.\n",
    "* **`Restaurant` (-0.41) & `Construction` (-0.40):** The presence of these major industries in the bottom 5 is a \"sanity check\" for our method. These sectors are known for high turnover and income volatility. The model now correctly quantifies that working in Construction is statistically \"riskier\" than working for the Government (WoE -0.40 vs +0.53)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Batch Weight of Evidence (WoE) Transformation**\n",
    "\n",
    "We have successfully tested the WoE logic on `ORGANIZATION_TYPE`. Now, our objective is to scale this solution. We have other complex categorical columns—specifically `OCCUPATION_TYPE` and `most_freq_seller_industry_prev`—that suffer from the same issues: too many categories to One-Hot Encode and potentially valuable patterns hidden in the missing values.\n",
    "\n",
    "My goal in this code is to automate the risk scoring for these columns. Instead of writing separate code blocks for each one, I am creating a robust loop that:\n",
    "\n",
    "1. **Calculates Risk Scores:** Converts text labels (e.g., \"Accountants\") into numeric WoE values.\n",
    "2. **Captures the \"Silence\":** This is the most critical technical detail. Instead of deleting missing values or guessing what they are, I am explicitly filling them with the text `\"MISSING\"`.\n",
    "* *Hypothesis:* In credit risk, an applicant who *refuses* to state their occupation might be riskier than one who proudly lists \"Accountant.\" By treating \"MISSING\" as its own category, the WoE formula will calculate a specific risk score for \"people who didn't tell us.\"\n",
    "\n",
    "\n",
    "3. **Applies Smoothing:** We continue to use Laplace Smoothing (+0.5) to ensure that small categories don't break the model.\n",
    "\n",
    "By the end of this loop, these columns will be fully converted into numeric features, ready for the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:42:59.456202Z",
     "iopub.status.busy": "2025-12-27T19:42:59.455395Z",
     "iopub.status.idle": "2025-12-27T19:42:59.828289Z",
     "shell.execute_reply": "2025-12-27T19:42:59.827529Z",
     "shell.execute_reply.started": "2025-12-27T19:42:59.456173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting WoE Transformation for 2 columns...\n",
      "   Creating new column: OCCUPATION_TYPE_WoE\n",
      "       Done. Range: -0.86 to 0.55\n",
      "      [Safest]   ['Accountants', 'High skill tech staff', 'Managers']\n",
      "      [Riskiest] ['Low-skill Laborers', 'Drivers', 'Waiters/barmen staff']\n",
      "--------------------------------------------------\n",
      "   Creating new column: most_freq_seller_industry_prev_WoE\n",
      "       Done. Range: -0.27 to 0.88\n",
      "      [Safest]   ['Tourism', 'Clothing', 'MLM partners']\n",
      "      [Riskiest] ['Auto technology', 'Jewelry', 'Connectivity']\n",
      "--------------------------------------------------\n",
      "\n",
      "All WoE transformations complete.\n"
     ]
    }
   ],
   "source": [
    "# List of columns to transform\n",
    "cols_to_woe = [\"OCCUPATION_TYPE\", \"most_freq_seller_industry_prev\"]\n",
    "\n",
    "print(f\" Starting WoE Transformation for {len(cols_to_woe)} columns...\")\n",
    "\n",
    "# 1. CALCULATE GLOBAL COUNTS (Needed for the formula)\n",
    "total_goods = df.filter(pl.col(\"TARGET\") == 0).height\n",
    "total_bads = df.filter(pl.col(\"TARGET\") == 1).height\n",
    "\n",
    "# 2. LOOP THROUGH COLUMNS\n",
    "for col in cols_to_woe:\n",
    "    woe_col_name = f\"{col}_WoE\"\n",
    "    \n",
    "    # --- A. CLEANUP (Prevent DuplicateError) ---\n",
    "    if woe_col_name in df.columns:\n",
    "        print(f\"   ⟳  Refreshing existing column: {woe_col_name}\")\n",
    "        df = df.drop(woe_col_name)\n",
    "    else:\n",
    "        print(f\"   Creating new column: {woe_col_name}\")\n",
    "\n",
    "    # --- B. HANDLE MISSING VALUES ---\n",
    "    # We treat 'Null' as a specific category called 'MISSING' so it gets a score.\n",
    "    # (We cast to String first to ensure we can modify it)\n",
    "    if df.schema[col] != pl.String:\n",
    "        df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "        \n",
    "    df = df.with_columns(pl.col(col).fill_null(\"MISSING\"))\n",
    "\n",
    "    # --- C. CALCULATE STATS ---\n",
    "    woe_stats = (\n",
    "        df.group_by(col)\n",
    "        .agg([\n",
    "            pl.len().alias(\"n_total\"),\n",
    "            pl.col(\"TARGET\").sum().alias(\"n_bad\"),\n",
    "            (pl.len() - pl.col(\"TARGET\").sum()).alias(\"n_good\")\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # --- D. APPLY FORMULA (With Smoothing) ---\n",
    "    woe_stats = woe_stats.with_columns(\n",
    "        (pl.col(\"n_good\") + 0.5).alias(\"n_good_smooth\"),\n",
    "        (pl.col(\"n_bad\") + 0.5).alias(\"n_bad_smooth\")\n",
    "    ).with_columns(\n",
    "        (np.log(\n",
    "            (pl.col(\"n_good_smooth\") / total_goods) / \n",
    "            (pl.col(\"n_bad_smooth\") / total_bads)\n",
    "        )).alias(woe_col_name)\n",
    "    )\n",
    "\n",
    "    # --- E. JOIN BACK ---\n",
    "    df = df.join(\n",
    "        woe_stats.select([col, woe_col_name]),\n",
    "        on=col,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # --- F. SHOW REPORT ---\n",
    "    print(f\"       Done. Range: {df[woe_col_name].min():.2f} to {df[woe_col_name].max():.2f}\")\n",
    "    \n",
    "    # Show Top 3 Safest vs Riskiest\n",
    "    print(f\"      [Safest]   {df.select([col, woe_col_name]).unique().sort(woe_col_name, descending=True).head(3)[col].to_list()}\")\n",
    "    print(f\"      [Riskiest] {df.select([col, woe_col_name]).unique().sort(woe_col_name, descending=False).head(3)[col].to_list()}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nAll WoE transformations complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our batch loop are excellent. We have successfully extracted strong risk signals from both categorical columns, and the specific rankings align perfectly with credit risk intuition.\n",
    "\n",
    "**1. Occupation Type: The Socioeconomic Ladder**\n",
    "The `OCCUPATION_TYPE_WoE` scores show a massive range (from `-0.86` to `+0.55`), confirming this is a critical predictor.\n",
    "\n",
    "* **The Safest (+WoE):** `Accountants`, `High skill tech staff`, and `Managers`. These are white-collar roles typically associated with higher, stable salaries and financial literacy. The model now \"knows\" these are safe bets.\n",
    "* **The Riskiest (-WoE):** `Low-skill Laborers` (`-0.86`) is the single riskiest category we've found so far. `Drivers` and `Waiters` also show negative scores. These roles often involve variable shifts, tips-based income, or lower job security, which correlates with default risk.\n",
    "\n",
    "**2. Previous Seller Industry: Spending Habits**\n",
    "The `most_freq_seller_industry_prev_WoE` feature tells us about the applicant's past consumption behavior.\n",
    "\n",
    "* **The Riskiest (-WoE):** `Connectivity` (likely mobile phones/electronics) and `Jewelry`. This is a classic pattern in consumer finance: people taking out loans specifically to buy the latest iPhone or expensive jewelry are statistically much more likely to default than those borrowing for other needs.\n",
    "* **The Safest (+WoE):** `Tourism` and `Clothing`. Applicants who have previously taken loans for travel (Tourism) likely have disposable income, signaling financial health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dropping Redundant Categorical Features**\n",
    "\n",
    "**Aim:**\n",
    "We have successfully extracted the risk signal from our complex categorical variables (`ORGANIZATION_TYPE`, `OCCUPATION_TYPE`, etc.) and stored it in our new numeric `_WoE` columns.\n",
    "\n",
    "My objective here is to **remove redundancy**. The original text columns are now \"dead weight\"—they contain the same information as the WoE columns but in a format (strings) that the model cannot understand without further processing.\n",
    "\n",
    "* **Efficiency:** Keeping them would be wasteful. If we kept them, we would have to One-Hot Encode them later, which would undo all our hard work by exploding the dataset size again.\n",
    "* **Verification:** This code also serves as a final safety check. I print the list of `_WoE` columns to confirm that while I am deleting the *raw* data, I am definitely keeping the *engineered* features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:43:47.243965Z",
     "iopub.status.busy": "2025-12-27T19:43:47.243149Z",
     "iopub.status.idle": "2025-12-27T19:43:47.250280Z",
     "shell.execute_reply": "2025-12-27T19:43:47.249529Z",
     "shell.execute_reply.started": "2025-12-27T19:43:47.243935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 3 original categorical columns...\n",
      "Drop complete.\n",
      "\n",
      "Remaining WoE Features: ['ORGANIZATION_TYPE_WoE', 'OCCUPATION_TYPE_WoE', 'most_freq_seller_industry_prev_WoE']\n",
      "Current DataFrame Shape: (307511, 146)\n"
     ]
    }
   ],
   "source": [
    "# 1. Define columns to remove (Original Categoricals)\n",
    "cols_to_remove = [\n",
    "    \"ORGANIZATION_TYPE\", \n",
    "    \"OCCUPATION_TYPE\", \n",
    "    \"most_freq_seller_industry_prev\"\n",
    "]\n",
    "\n",
    "print(f\"Dropping {len(cols_to_remove)} original categorical columns...\")\n",
    "\n",
    "# 2. Execute Drop\n",
    "# We check existence first to prevent errors if you run this cell twice\n",
    "existing_cols_to_drop = [c for c in cols_to_remove if c in df.columns]\n",
    "\n",
    "if existing_cols_to_drop:\n",
    "    df = df.drop(existing_cols_to_drop)\n",
    "    print(\"Drop complete.\")\n",
    "else:\n",
    "    print(\"Columns already dropped.\")\n",
    "\n",
    "# 3. Verify the WoE columns remain\n",
    "woe_cols = [c for c in df.columns if \"_WoE\" in c]\n",
    "print(f\"\\nRemaining WoE Features: {woe_cols}\")\n",
    "print(f\"Current DataFrame Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Label Encoding for Remaining Categoricals**\n",
    "\n",
    "**Aim**\n",
    "We have already handled the \"difficult\" high-cardinality columns using WoE. However, we still have several \"simple\" categorical features remaining in the dataset, such as `CODE_GENDER`, `NAME_FAMILY_STATUS`, and `FLAG_OWN_CAR`.\n",
    "\n",
    "My objective here is to convert these remaining text columns into integers (0, 1, 2...) so the machine learning model can process them.\n",
    "\n",
    "* **Method:** I am using **Label Encoding** (via Polars' `.to_physical()` method).  Unlike One-Hot Encoding, this does not create new columns. It simply maps \"Male\" to `0` and \"Female\" to `1`.\n",
    "* **Why this works:** Modern tree-based models like LightGBM and XGBoost are smart enough to handle integer-encoded categories natively. They don't need the data to be expanded into dummy variables.\n",
    "* **Handling Nulls:** Instead of leaving holes in the data, I fill missing values with `\"MISSING_CATEGORY\"` before encoding. This assigns a specific integer ID to \"Missing,\" allowing the model to learn if *not answering a question* is itself a risk factor.\n",
    "\n",
    "The final block acts as a \"Gatekeeper\": it scans the entire dataset one last time to confirm that **zero** string columns remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:43:50.568429Z",
     "iopub.status.busy": "2025-12-27T19:43:50.567551Z",
     "iopub.status.idle": "2025-12-27T19:43:50.712037Z",
     "shell.execute_reply": "2025-12-27T19:43:50.711211Z",
     "shell.execute_reply.started": "2025-12-27T19:43:50.568391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 categorical columns to encode: ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'last_rejection_reason_prev']\n",
      "\n",
      "ENCODING COMPLETE.\n",
      "------------------------------\n",
      "SUCCESS: Dataset is 100% Numeric. Ready for LightGBM/XGBoost.\n",
      "Final Data Shape: (307511, 146)\n"
     ]
    }
   ],
   "source": [
    "# 1. Identify Remaining Categorical Columns\n",
    "# We verify ensuring we don't accidentally encode numeric cols\n",
    "cat_cols = [\n",
    "    c for c, t in df.schema.items() \n",
    "    if t in (pl.String, pl.Object, pl.Categorical)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(cat_cols)} categorical columns to encode: {cat_cols}\")\n",
    "\n",
    "# 2. Apply Label Encoding (Preserving Nulls)\n",
    "for col in cat_cols:\n",
    "    # A. Fill Nulls with specific placeholder (Captures the \"Missing Risk\")\n",
    "    # B. Cast to Categorical (Polars creates the mapping)\n",
    "    # C. Cast to Physical (Converts to integer IDs: 0, 1, 2...)\n",
    "    df = df.with_columns(\n",
    "        pl.col(col)\n",
    "        .fill_null(\"MISSING_CATEGORY\") \n",
    "        .cast(pl.Categorical)\n",
    "        .to_physical()\n",
    "        .alias(col)\n",
    "    )\n",
    "\n",
    "# 3. Final Verification\n",
    "print(\"\\nENCODING COMPLETE.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check for any remaining non-numeric columns\n",
    "non_numeric = [c for c, t in df.schema.items() if t == pl.String or t == pl.Object]\n",
    "\n",
    "if len(non_numeric) == 0:\n",
    "    print(\"SUCCESS: Dataset is 100% Numeric. Ready for LightGBM/XGBoost.\")\n",
    "else:\n",
    "    print(f\"WARNING: Some columns remain non-numeric: {non_numeric}\")\n",
    "\n",
    "print(f\"Final Data Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T14:59:25.776370Z",
     "iopub.status.busy": "2025-12-27T14:59:25.775819Z",
     "iopub.status.idle": "2025-12-27T14:59:25.784406Z",
     "shell.execute_reply": "2025-12-27T14:59:25.783765Z",
     "shell.execute_reply.started": "2025-12-27T14:59:25.776342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 128)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>SK_ID_CURR</th><th>TARGET</th><th>NAME_CONTRACT_TYPE</th><th>CODE_GENDER</th><th>FLAG_OWN_CAR</th><th>FLAG_OWN_REALTY</th><th>CNT_CHILDREN</th><th>AMT_INCOME_TOTAL</th><th>AMT_CREDIT</th><th>AMT_ANNUITY</th><th>AMT_GOODS_PRICE</th><th>NAME_TYPE_SUITE</th><th>NAME_INCOME_TYPE</th><th>NAME_EDUCATION_TYPE</th><th>NAME_FAMILY_STATUS</th><th>NAME_HOUSING_TYPE</th><th>REGION_POPULATION_RELATIVE</th><th>DAYS_BIRTH</th><th>DAYS_EMPLOYED</th><th>DAYS_REGISTRATION</th><th>DAYS_ID_PUBLISH</th><th>OWN_CAR_AGE</th><th>FLAG_WORK_PHONE</th><th>FLAG_PHONE</th><th>CNT_FAM_MEMBERS</th><th>REGION_RATING_CLIENT</th><th>REGION_RATING_CLIENT_W_CITY</th><th>WEEKDAY_APPR_PROCESS_START</th><th>HOUR_APPR_PROCESS_START</th><th>REG_CITY_NOT_LIVE_CITY</th><th>EXT_SOURCE_1</th><th>EXT_SOURCE_2</th><th>EXT_SOURCE_3</th><th>FONDKAPREMONT_MODE</th><th>HOUSETYPE_MODE</th><th>TOTALAREA_MODE</th><th>WALLSMATERIAL_MODE</th><th>&hellip;</th><th>ins_late_payment_count</th><th>ins_avg_dpd</th><th>ins_max_dpd</th><th>ins_avg_dbd</th><th>ins_max_dbd</th><th>ins_underpayment_count</th><th>ins_payment_ratio</th><th>ins_total_missed_balance</th><th>ins_version_change_count</th><th>ins_recent_late_count_1y</th><th>bureau_total_loans_count</th><th>bureau_active_loans_count</th><th>bureau_closed_loans_count</th><th>bureau_credit_card_count</th><th>bureau_mortgage_count</th><th>bureau_car_loan_count</th><th>bureau_total_active_debt</th><th>bureau_total_active_limit</th><th>bureau_total_overdue_amount</th><th>bureau_max_overdue_ever</th><th>bureau_utilization_rate</th><th>bureau_days_since_last_loan</th><th>bureau_days_until_next_end</th><th>bureau_max_delinquency_level_ever</th><th>bureau_total_late_months_history</th><th>ratio_credit_to_income</th><th>ratio_annuity_to_income</th><th>ratio_annuity_to_credit</th><th>ratio_total_debt_to_income</th><th>ratio_total_credit_limit_to_income</th><th>feature_disposable_income</th><th>ratio_employed_to_age</th><th>ratio_income_to_age</th><th>ORGANIZATION_TYPE_WoE_right</th><th>ORGANIZATION_TYPE_WoE</th><th>OCCUPATION_TYPE_WoE</th><th>most_freq_seller_industry_prev_WoE</th></tr><tr><td>i64</td><td>i64</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>u32</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>u32</td><td>u32</td><td>f64</td><td>u32</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>i32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>384405</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>126000.0</td><td>780363.0</td><td>31077.0</td><td>697500.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.032561</td><td>-18334</td><td>-2918</td><td>-1345.0</td><td>-1804</td><td>15.0</td><td>0</td><td>0</td><td>2.0</td><td>1</td><td>1</td><td>0</td><td>17</td><td>0</td><td>null</td><td>0.704051</td><td>null</td><td>0</td><td>0</td><td>0.0553</td><td>0</td><td>&hellip;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>38.315789</td><td>71.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2</td><td>0.0</td><td>1</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.275228e6</td><td>1.35e6</td><td>0.0</td><td>null</td><td>0.944613</td><td>-308</td><td>null</td><td>null</td><td>0.0</td><td>6.193357</td><td>0.246643</td><td>0.039824</td><td>16.314214</td><td>16.907643</td><td>94923.0</td><td>0.159158</td><td>6.872477</td><td>0.156938</td><td>0.156938</td><td>0.290375</td><td>-0.102228</td></tr><tr><td>384407</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>112500.0</td><td>331834.5</td><td>20182.5</td><td>252000.0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0.005084</td><td>-11368</td><td>-2860</td><td>-3625.0</td><td>-3506</td><td>null</td><td>0</td><td>0</td><td>2.0</td><td>2</td><td>2</td><td>0</td><td>10</td><td>1</td><td>0.489432</td><td>0.745284</td><td>0.406617</td><td>1</td><td>1</td><td>null</td><td>1</td><td>&hellip;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>13.7</td><td>39.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2</td><td>0.0</td><td>3</td><td>3.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>0.0</td><td>173700.0</td><td>387000.0</td><td>0.0</td><td>null</td><td>0.448837</td><td>-60</td><td>1510.0</td><td>0</td><td>0.0</td><td>2.94964</td><td>0.1794</td><td>0.060821</td><td>4.49364</td><td>6.38964</td><td>92317.5</td><td>0.251583</td><td>9.8962</td><td>-0.303195</td><td>-0.303195</td><td>0.281369</td><td>0.119861</td></tr><tr><td>384408</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>202500.0</td><td>545040.0</td><td>20677.5</td><td>450000.0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.010276</td><td>-18286</td><td>-2789</td><td>-10980.0</td><td>-1821</td><td>null</td><td>0</td><td>0</td><td>2.0</td><td>2</td><td>2</td><td>1</td><td>13</td><td>0</td><td>null</td><td>0.533752</td><td>0.540654</td><td>1</td><td>1</td><td>null</td><td>1</td><td>&hellip;</td><td>1.0</td><td>0.058824</td><td>3.0</td><td>8.372549</td><td>60.0</td><td>2.0</td><td>1.152014</td><td>-174219.93</td><td>4</td><td>1.0</td><td>3</td><td>2.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>230184.0</td><td>396000.0</td><td>0.0</td><td>0.0</td><td>0.581273</td><td>-667</td><td>681.0</td><td>0</td><td>0.0</td><td>2.691556</td><td>0.102111</td><td>0.037938</td><td>3.828267</td><td>4.647111</td><td>181822.5</td><td>0.152521</td><td>11.074046</td><td>0.059096</td><td>0.059096</td><td>-0.315772</td><td>-0.102228</td></tr><tr><td>384409</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>194121.0</td><td>1.35e6</td><td>48798.0</td><td>1.35e6</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0.030755</td><td>-14060</td><td>-948</td><td>-264.0</td><td>-4417</td><td>6.0</td><td>1</td><td>0</td><td>2.0</td><td>2</td><td>2</td><td>2</td><td>18</td><td>1</td><td>null</td><td>0.599934</td><td>0.634706</td><td>1</td><td>1</td><td>null</td><td>1</td><td>&hellip;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>17.857143</td><td>37.0</td><td>0.0</td><td>1.510805</td><td>-102844.26</td><td>4</td><td>0.0</td><td>1</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-516</td><td>-455.0</td><td>null</td><td>0.0</td><td>6.954425</td><td>0.251379</td><td>0.036147</td><td>6.954425</td><td>6.954425</td><td>145323.0</td><td>0.067425</td><td>13.806615</td><td>0.150734</td><td>0.150734</td><td>0.281369</td><td>0.119861</td></tr><tr><td>384410</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>112500.0</td><td>296505.0</td><td>23904.0</td><td>247500.0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.035792</td><td>-17274</td><td>-614</td><td>-2674.0</td><td>-809</td><td>null</td><td>0</td><td>0</td><td>2.0</td><td>2</td><td>2</td><td>3</td><td>11</td><td>0</td><td>null</td><td>0.367819</td><td>null</td><td>0</td><td>0</td><td>0.1129</td><td>2</td><td>&hellip;</td><td>6.0</td><td>0.433962</td><td>6.0</td><td>10.830189</td><td>40.0</td><td>10.0</td><td>0.954487</td><td>9044.19</td><td>4</td><td>4.0</td><td>2</td><td>0.0</td><td>2.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>-524</td><td>1303.0</td><td>1</td><td>4.0</td><td>2.6356</td><td>0.21248</td><td>0.080619</td><td>2.6356</td><td>2.6356</td><td>88596.0</td><td>0.035545</td><td>6.512678</td><td>-0.173159</td><td>-0.173159</td><td>-0.193808</td><td>-0.102228</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 128)\n",
       "┌────────────┬────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ SK_ID_CURR ┆ TARGET ┆ NAME_CONTR ┆ CODE_GEND ┆ … ┆ ORGANIZAT ┆ ORGANIZAT ┆ OCCUPATIO ┆ most_freq │\n",
       "│ ---        ┆ ---    ┆ ACT_TYPE   ┆ ER        ┆   ┆ ION_TYPE_ ┆ ION_TYPE_ ┆ N_TYPE_Wo ┆ _seller_i │\n",
       "│ i64        ┆ i64    ┆ ---        ┆ ---       ┆   ┆ WoE_right ┆ WoE       ┆ E         ┆ ndustry_p │\n",
       "│            ┆        ┆ u32        ┆ u32       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ rev…      │\n",
       "│            ┆        ┆            ┆           ┆   ┆ f64       ┆ f64       ┆ f64       ┆ ---       │\n",
       "│            ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆ f64       │\n",
       "╞════════════╪════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 384405     ┆ 0      ┆ 0          ┆ 0         ┆ … ┆ 0.156938  ┆ 0.156938  ┆ 0.290375  ┆ -0.102228 │\n",
       "│ 384407     ┆ 0      ┆ 0          ┆ 0         ┆ … ┆ -0.303195 ┆ -0.303195 ┆ 0.281369  ┆ 0.119861  │\n",
       "│ 384408     ┆ 0      ┆ 0          ┆ 0         ┆ … ┆ 0.059096  ┆ 0.059096  ┆ -0.315772 ┆ -0.102228 │\n",
       "│ 384409     ┆ 0      ┆ 0          ┆ 1         ┆ … ┆ 0.150734  ┆ 0.150734  ┆ 0.281369  ┆ 0.119861  │\n",
       "│ 384410     ┆ 0      ┆ 0          ┆ 0         ┆ … ┆ -0.173159 ┆ -0.173159 ┆ -0.193808 ┆ -0.102228 │\n",
       "└────────────┴────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Cleanup and WoE Verification**\n",
    "\n",
    "**Aim**\n",
    "After performing multiple merge operations to add our new Weight of Evidence (WoE) features, we must check for \"merge artifacts.\" In dataframe operations, joining tables can sometimes accidentally create duplicate columns (often suffixed with `_right` or `_dup`) if the join keys aren't perfectly handled.\n",
    "\n",
    "My objective here is to:\n",
    "\n",
    "1. **Remove Duplicates:** I am running a scan to automatically find and delete any columns ending in `_right`. This ensures the dataset is clean and we don't have two versions of the same variable confusing the model.\n",
    "2. **Verify Feature Engineering:** I explicitly check that our three critical new features (`ORGANIZATION_TYPE_WoE`, `OCCUPATION_TYPE_WoE`, etc.) are actually present in the final dataframe. This is a \"Sanity Check\"—if these columns are missing, it means our previous engineering steps failed silently, and we need to know that *now* before we try to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:43:58.076538Z",
     "iopub.status.busy": "2025-12-27T19:43:58.076004Z",
     "iopub.status.idle": "2025-12-27T19:43:58.082577Z",
     "shell.execute_reply": "2025-12-27T19:43:58.081807Z",
     "shell.execute_reply.started": "2025-12-27T19:43:58.076508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No '_right' duplicates found.\n",
      "\n",
      "--- Final WoE Column Check ---\n",
      "ORGANIZATION_TYPE_WoE is present.\n",
      "OCCUPATION_TYPE_WoE is present.\n",
      "most_freq_seller_industry_prev_WoE is present.\n",
      "\n",
      "Final Shape: (307511, 146)\n"
     ]
    }
   ],
   "source": [
    "# 1. Identify Columns to Drop (Ending in '_right')\n",
    "cols_to_drop = [c for c in df.columns if c.endswith(\"_right\")]\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"Found duplicate columns to clean: {cols_to_drop}\")\n",
    "    df = df.drop(cols_to_drop)\n",
    "    print(\"Duplicates removed.\")\n",
    "else:\n",
    "    print(\"No '_right' duplicates found.\")\n",
    "\n",
    "# 2. Verify Final WoE Columns\n",
    "# We want to make sure we have exactly one of each expected WoE column\n",
    "expected_woe = [\n",
    "    \"ORGANIZATION_TYPE_WoE\", \n",
    "    \"OCCUPATION_TYPE_WoE\", \n",
    "    \"most_freq_seller_industry_prev_WoE\"\n",
    "]\n",
    "\n",
    "print(\"\\n--- Final WoE Column Check ---\")\n",
    "for col in expected_woe:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col} is present.\")\n",
    "    else:\n",
    "        print(f\"WARNING: {col} is MISSING. You may need to re-run the WoE calculation cell.\")\n",
    "\n",
    "print(f\"\\nFinal Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:44:07.768548Z",
     "iopub.status.busy": "2025-12-27T19:44:07.767978Z",
     "iopub.status.idle": "2025-12-27T19:44:07.777723Z",
     "shell.execute_reply": "2025-12-27T19:44:07.776931Z",
     "shell.execute_reply.started": "2025-12-27T19:44:07.768511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 146)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>SK_ID_CURR</th><th>TARGET</th><th>NAME_CONTRACT_TYPE</th><th>CODE_GENDER</th><th>FLAG_OWN_CAR</th><th>FLAG_OWN_REALTY</th><th>CNT_CHILDREN</th><th>AMT_INCOME_TOTAL</th><th>AMT_CREDIT</th><th>AMT_ANNUITY</th><th>AMT_GOODS_PRICE</th><th>NAME_TYPE_SUITE</th><th>NAME_INCOME_TYPE</th><th>NAME_EDUCATION_TYPE</th><th>NAME_FAMILY_STATUS</th><th>NAME_HOUSING_TYPE</th><th>REGION_POPULATION_RELATIVE</th><th>DAYS_BIRTH</th><th>DAYS_EMPLOYED</th><th>DAYS_REGISTRATION</th><th>DAYS_ID_PUBLISH</th><th>OWN_CAR_AGE</th><th>FLAG_MOBIL</th><th>FLAG_EMP_PHONE</th><th>FLAG_WORK_PHONE</th><th>FLAG_CONT_MOBILE</th><th>FLAG_PHONE</th><th>FLAG_EMAIL</th><th>CNT_FAM_MEMBERS</th><th>REGION_RATING_CLIENT</th><th>REGION_RATING_CLIENT_W_CITY</th><th>WEEKDAY_APPR_PROCESS_START</th><th>HOUR_APPR_PROCESS_START</th><th>REG_REGION_NOT_LIVE_REGION</th><th>REG_REGION_NOT_WORK_REGION</th><th>LIVE_REGION_NOT_WORK_REGION</th><th>REG_CITY_NOT_LIVE_CITY</th><th>&hellip;</th><th>pos_term_variance</th><th>ins_late_payment_count</th><th>ins_avg_dpd</th><th>ins_max_dpd</th><th>ins_avg_dbd</th><th>ins_max_dbd</th><th>ins_underpayment_count</th><th>ins_payment_ratio</th><th>ins_total_missed_balance</th><th>ins_version_change_count</th><th>ins_recent_late_count_1y</th><th>bureau_total_loans_count</th><th>bureau_active_loans_count</th><th>bureau_closed_loans_count</th><th>bureau_credit_card_count</th><th>bureau_mortgage_count</th><th>bureau_car_loan_count</th><th>bureau_total_active_debt</th><th>bureau_total_active_limit</th><th>bureau_total_overdue_amount</th><th>bureau_max_overdue_ever</th><th>bureau_utilization_rate</th><th>bureau_days_since_last_loan</th><th>bureau_days_until_next_end</th><th>bureau_max_delinquency_level_ever</th><th>bureau_total_late_months_history</th><th>ratio_credit_to_income</th><th>ratio_annuity_to_income</th><th>ratio_annuity_to_credit</th><th>ratio_total_debt_to_income</th><th>ratio_total_credit_limit_to_income</th><th>feature_disposable_income</th><th>ratio_employed_to_age</th><th>ratio_income_to_age</th><th>ORGANIZATION_TYPE_WoE</th><th>OCCUPATION_TYPE_WoE</th><th>most_freq_seller_industry_prev_WoE</th></tr><tr><td>i64</td><td>i64</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>u32</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>i32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>384405</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>126000.0</td><td>780363.0</td><td>31077.0</td><td>697500.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.032561</td><td>-18334</td><td>-2918</td><td>-1345.0</td><td>-1804</td><td>15.0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>2.0</td><td>1</td><td>1</td><td>0</td><td>17</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>2.260526</td><td>0.0</td><td>0.0</td><td>0.0</td><td>38.315789</td><td>71.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2</td><td>0.0</td><td>1</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.275228e6</td><td>1.35e6</td><td>0.0</td><td>null</td><td>0.944613</td><td>-308</td><td>null</td><td>null</td><td>0.0</td><td>6.193357</td><td>0.246643</td><td>0.039824</td><td>16.314214</td><td>16.907643</td><td>94923.0</td><td>0.159158</td><td>6.872477</td><td>0.156938</td><td>0.290375</td><td>-0.102228</td></tr><tr><td>384407</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>112500.0</td><td>331834.5</td><td>20182.5</td><td>252000.0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0.005084</td><td>-11368</td><td>-2860</td><td>-3625.0</td><td>-3506</td><td>null</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>2.0</td><td>2</td><td>2</td><td>0</td><td>10</td><td>1</td><td>1</td><td>0</td><td>1</td><td>&hellip;</td><td>4.69697</td><td>0.0</td><td>0.0</td><td>0.0</td><td>13.7</td><td>39.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2</td><td>0.0</td><td>3</td><td>3.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>0.0</td><td>173700.0</td><td>387000.0</td><td>0.0</td><td>null</td><td>0.448837</td><td>-60</td><td>1510.0</td><td>0</td><td>0.0</td><td>2.94964</td><td>0.1794</td><td>0.060821</td><td>4.49364</td><td>6.38964</td><td>92317.5</td><td>0.251583</td><td>9.8962</td><td>-0.303195</td><td>0.281369</td><td>0.119861</td></tr><tr><td>384408</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>202500.0</td><td>545040.0</td><td>20677.5</td><td>450000.0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.010276</td><td>-18286</td><td>-2789</td><td>-10980.0</td><td>-1821</td><td>null</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>2.0</td><td>2</td><td>2</td><td>1</td><td>13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>16.457143</td><td>1.0</td><td>0.058824</td><td>3.0</td><td>8.372549</td><td>60.0</td><td>2.0</td><td>1.152014</td><td>-174219.93</td><td>4</td><td>1.0</td><td>3</td><td>2.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>230184.0</td><td>396000.0</td><td>0.0</td><td>0.0</td><td>0.581273</td><td>-667</td><td>681.0</td><td>0</td><td>0.0</td><td>2.691556</td><td>0.102111</td><td>0.037938</td><td>3.828267</td><td>4.647111</td><td>181822.5</td><td>0.152521</td><td>11.074046</td><td>0.059096</td><td>-0.315772</td><td>-0.102228</td></tr><tr><td>384409</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>194121.0</td><td>1.35e6</td><td>48798.0</td><td>1.35e6</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0.030755</td><td>-14060</td><td>-948</td><td>-264.0</td><td>-4417</td><td>6.0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>2.0</td><td>2</td><td>2</td><td>2</td><td>18</td><td>1</td><td>1</td><td>1</td><td>1</td><td>&hellip;</td><td>78.929167</td><td>0.0</td><td>0.0</td><td>0.0</td><td>17.857143</td><td>37.0</td><td>0.0</td><td>1.510805</td><td>-102844.26</td><td>4</td><td>0.0</td><td>1</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-516</td><td>-455.0</td><td>null</td><td>0.0</td><td>6.954425</td><td>0.251379</td><td>0.036147</td><td>6.954425</td><td>6.954425</td><td>145323.0</td><td>0.067425</td><td>13.806615</td><td>0.150734</td><td>0.281369</td><td>0.119861</td></tr><tr><td>384410</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>112500.0</td><td>296505.0</td><td>23904.0</td><td>247500.0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.035792</td><td>-17274</td><td>-614</td><td>-2674.0</td><td>-809</td><td>null</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>2.0</td><td>2</td><td>2</td><td>3</td><td>11</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0.0</td><td>6.0</td><td>0.433962</td><td>6.0</td><td>10.830189</td><td>40.0</td><td>10.0</td><td>0.954487</td><td>9044.19</td><td>4</td><td>4.0</td><td>2</td><td>0.0</td><td>2.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>-524</td><td>1303.0</td><td>1</td><td>4.0</td><td>2.6356</td><td>0.21248</td><td>0.080619</td><td>2.6356</td><td>2.6356</td><td>88596.0</td><td>0.035545</td><td>6.512678</td><td>-0.173159</td><td>-0.193808</td><td>-0.102228</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 146)\n",
       "┌────────────┬────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ SK_ID_CURR ┆ TARGET ┆ NAME_CONTR ┆ CODE_GEND ┆ … ┆ ratio_inc ┆ ORGANIZAT ┆ OCCUPATIO ┆ most_freq │\n",
       "│ ---        ┆ ---    ┆ ACT_TYPE   ┆ ER        ┆   ┆ ome_to_ag ┆ ION_TYPE_ ┆ N_TYPE_Wo ┆ _seller_i │\n",
       "│ i64        ┆ i64    ┆ ---        ┆ ---       ┆   ┆ e         ┆ WoE       ┆ E         ┆ ndustry_p │\n",
       "│            ┆        ┆ u32        ┆ u32       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ rev…      │\n",
       "│            ┆        ┆            ┆           ┆   ┆ f64       ┆ f64       ┆ f64       ┆ ---       │\n",
       "│            ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆ f64       │\n",
       "╞════════════╪════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 384405     ┆ 0      ┆ 0          ┆ 0         ┆ … ┆ 6.872477  ┆ 0.156938  ┆ 0.290375  ┆ -0.102228 │\n",
       "│ 384407     ┆ 0      ┆ 0          ┆ 0         ┆ … ┆ 9.8962    ┆ -0.303195 ┆ 0.281369  ┆ 0.119861  │\n",
       "│ 384408     ┆ 0      ┆ 0          ┆ 0         ┆ … ┆ 11.074046 ┆ 0.059096  ┆ -0.315772 ┆ -0.102228 │\n",
       "│ 384409     ┆ 0      ┆ 0          ┆ 1         ┆ … ┆ 13.806615 ┆ 0.150734  ┆ 0.281369  ┆ 0.119861  │\n",
       "│ 384410     ┆ 0      ┆ 0          ┆ 0         ┆ … ┆ 6.512678  ┆ -0.173159 ┆ -0.193808 ┆ -0.102228 │\n",
       "└────────────┴────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Data Readiness Scan**\n",
    "\n",
    "**Aim**\n",
    "We have finished all our cleaning, encoding, and feature engineering. However, before we feed this large dataset into a sensitive algorithm like LightGBM or XGBoost, we must perform a **\"Pre-Flight Check\"** (Unit Test). Machine learning models are fragile; a single \"String\" hidden in column 100 or a single \"Infinity\" value can cause the entire training process to crash after hours of computation.\n",
    "\n",
    "My objective here is to automatically verify three critical technical constraints:\n",
    "\n",
    "1. **Type Consistency (The Numeric Check):**\n",
    "* **What we do:** We scan every column to ensure it is strictly numeric (Float, Integer, or Boolean).\n",
    "* **Why:** If we missed even one categorical column (e.g., a \"Date\" string we forgot to drop), the model will throw a `TypeError` and fail immediately.\n",
    "\n",
    "\n",
    "2. **Mathematical Stability (The Infinity Check):**\n",
    "* **What we do:** We scan all floating-point columns for `inf` or `-inf` values.\n",
    "* **Why:** In our feature engineering (e.g., `ratio_annuity_to_income`), we might have divided by zero. If the model sees \"Infinity,\" it cannot calculate gradients (math breaks down), leading to `NaN` losses or crashes.\n",
    "\n",
    "\n",
    "3. **Target Validity (The Label Check):**\n",
    "* **What we do:** We verify that the `TARGET` column exists and contains exactly two classes: `0` (Repaid) and `1` (Default).\n",
    "* **Why:** This ensures we didn't accidentally drop our prediction label during the earlier cleaning steps. We also calculate the balance (e.g., 8% default rate) to confirm the data looks realistic.\n",
    "\n",
    "\n",
    "\n",
    "**System Status:**\n",
    "The final block acts as a master switch. It only prints **\"SYSTEM STATUS: GO\"** if all three checks pass. This gives us 100% confidence to proceed to the Model Training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:47:54.126196Z",
     "iopub.status.busy": "2025-12-27T19:47:54.125893Z",
     "iopub.status.idle": "2025-12-27T19:47:54.171180Z",
     "shell.execute_reply": "2025-12-27T19:47:54.170531Z",
     "shell.execute_reply.started": "2025-12-27T19:47:54.126170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING FAST DATA READINESS SCAN...\n",
      "\n",
      "TYPE CHECK: All columns are numeric.\n",
      "Checking for Infinity values (One-shot scan)...\n",
      "MATH CHECK: No infinite values found.\n",
      "TARGET CHECK: Valid Binary Target [0, 1]\n",
      "   Balance: 8.07% Default Rate (Safely Imbalanced)\n",
      "\n",
      "==============================\n",
      "SYSTEM STATUS: GO. READY FOR TRAINING.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "print(\"STARTING FAST DATA READINESS SCAN...\\n\")\n",
    "\n",
    "# 1. TYPE CHECK\n",
    "# ---------------------------------------------------------\n",
    "# We check if any column is NOT numeric (Number, Float, Int)\n",
    "non_numeric_cols = df.select(\n",
    "    ~cs.numeric() & ~cs.boolean() # Exclude numbers and booleans\n",
    ").columns\n",
    "\n",
    "if len(non_numeric_cols) > 0:\n",
    "    print(f\"CRITICAL FAIL: Found {len(non_numeric_cols)} non-numeric columns.\")\n",
    "    print(f\"   Must Encode/Drop: {non_numeric_cols}\")\n",
    "else:\n",
    "    print(\"TYPE CHECK: All columns are numeric.\")\n",
    "\n",
    "\n",
    "# 2. INFINITY CHECK (Vectorized)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Checking for Infinity values (One-shot scan)...\")\n",
    "\n",
    "# Only Floats can be infinite (Integers cannot), so we only scan Float columns\n",
    "float_cols = [c for c, t in df.schema.items() if t in (pl.Float32, pl.Float64)]\n",
    "\n",
    "if float_cols:\n",
    "    # Build a SINGLE query to sum infinity for all float columns at once\n",
    "    inf_stats = df.select([\n",
    "        pl.col(c).is_infinite().sum().alias(c) for c in float_cols\n",
    "    ])\n",
    "    \n",
    "    # Transpose to find the offenders\n",
    "    inf_summary = (\n",
    "        inf_stats.transpose(include_header=True, header_name=\"Column\", column_names=[\"Inf_Count\"])\n",
    "        .filter(pl.col(\"Inf_Count\") > 0)\n",
    "    )\n",
    "\n",
    "    if inf_summary.height > 0:\n",
    "        print(f\"MATH FAIL: Found {inf_summary.height} columns with Infinity.\")\n",
    "        print(\"   Run the 'Fix Infinity' block below.\")\n",
    "        with pl.Config(tbl_rows=10):\n",
    "            print(inf_summary)\n",
    "        inf_count = inf_summary.height\n",
    "    else:\n",
    "        print(\"MATH CHECK: No infinite values found.\")\n",
    "        inf_count = 0\n",
    "else:\n",
    "    print(\"MATH CHECK: No Float columns to check (all Integers).\")\n",
    "    inf_count = 0\n",
    "\n",
    "\n",
    "# 3. TARGET CHECK\n",
    "# ---------------------------------------------------------\n",
    "if \"TARGET\" in df.columns:\n",
    "    # Use fast aggregation instead of unique().to_list()\n",
    "    target_stats = df.group_by(\"TARGET\").len().sort(\"TARGET\")\n",
    "    \n",
    "    # Check if we have 0 and 1\n",
    "    targets = target_stats[\"TARGET\"].to_list()\n",
    "    \n",
    "    if targets == [0, 1] or targets == [0.0, 1.0]:\n",
    "        print(f\"TARGET CHECK: Valid Binary Target {targets}\")\n",
    "        \n",
    "        # Calculate Balance\n",
    "        count_0 = target_stats.filter(pl.col(\"TARGET\") == 0)[\"len\"][0]\n",
    "        count_1 = target_stats.filter(pl.col(\"TARGET\") == 1)[\"len\"][0]\n",
    "        ratio = count_1 / (count_0 + count_1)\n",
    "        print(f\"   Balance: {ratio:.2%} Default Rate (Safely Imbalanced)\")\n",
    "    else:\n",
    "        print(f\"TARGET FAIL: Strange values found: {targets}\")\n",
    "else:\n",
    "    print(\"TARGET FAIL: 'TARGET' column is MISSING!\")\n",
    "\n",
    "\n",
    "# 4. FINAL STATUS\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "if len(non_numeric_cols) == 0 and inf_count == 0 and \"TARGET\" in df.columns:\n",
    "    print(\"SYSTEM STATUS: GO. READY FOR TRAINING.\")\n",
    "else:\n",
    "    print(\"SYSTEM STATUS: NO_GO. FIX ERRORS ABOVE.\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save Cleaned Data to Parquet**\n",
    "\n",
    "**Aim**\n",
    "We have successfully cleaned, encoded, and validated our dataset. It is now ready for machine learning. However, we do not want to re-run this entire 15-step cleaning pipeline every time we want to train a model or tune hyperparameters.\n",
    "\n",
    "My objective here is to **\"Checkpoint\"** our work.\n",
    "\n",
    "* **Why Parquet?** I am saving the data as a `.parquet` file rather than a `.csv`. Parquet is a binary, columnar format that is significantly faster to read/write and, most importantly, **preserves data types**.\n",
    "* If I saved as CSV, my integer categories (0, 1, 2) might get reloaded as floats (0.0, 1.0), breaking the optimization. Parquet guarantees the schema remains identical.\n",
    "\n",
    "\n",
    "* **Compression:** Using `zstd` compression keeps the file size small without sacrificing read speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. SAVE CLEANED DATA\n",
    "output_path = \"home_credit_train_cleaned.parquet\"\n",
    "print(f\"Saving cleaned dataset to: {output_path} ...\")\n",
    "\n",
    "try:\n",
    "    # We use zstd compression for a good balance of speed and file size\n",
    "    df.write_parquet(output_path, compression=\"zstd\")\n",
    "    print(\"Save complete. Checkpoint created.\")\n",
    "    \n",
    "    # Optional: Verify file size to confirm it wrote something substantial\n",
    "    import os\n",
    "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f\"File Size: {size_mb:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9133603,
     "sourceId": 14307577,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
